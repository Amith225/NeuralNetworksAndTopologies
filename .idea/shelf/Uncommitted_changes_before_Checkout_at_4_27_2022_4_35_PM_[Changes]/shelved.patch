Index: test.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from DataSets import dataSet\r\nfrom NeuralNetworks import *\r\nfrom Topologies import *\r\nfrom Utils import *\r\ndb = DataBase.load(dataSet.TrainSets.EmnistBalanced, normalizeInp=1, reshapeInp=(-1, 1))\r\ndb2 = DataBase.load(dataSet.TestSets.EmnistBalanced, normalizeInp=1, reshapeInp=(-1, 1))\r\n\r\nhiddenShape = 392, 196\r\nnn = ArtificialNeuralNetwork(shape=Shape(db.inpShape[0], *hiddenShape, db.tarShape[0]),\r\n                             initializer=Xavier(2),\r\n                             activators=Activators(Prelu(),\r\n                                                   ...,\r\n                                                   Softmax()),\r\n                             costFunction=MeanSquareLossFunction())\r\n\r\nnn.train(1, 64,\r\n         trainDataBase=db,\r\n         optimizer=AdagradWBOptimizer(nn),\r\n         profile=False,\r\n         test=db2)\r\n\r\n# nn.save(replace=True)\r\n# PlotNeuralNetwork.plotCostGraph(nn)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/test.py b/test.py
--- a/test.py	
+++ b/test.py	
@@ -14,8 +14,8 @@
                              costFunction=MeanSquareLossFunction())
 
 nn.train(1, 64,
-         trainDataBase=db,
-         optimizer=AdagradWBOptimizer(nn),
+         trainDataBase=db2,
+         optimizer=GradientDecentWBOptimizer(nn, 0.001),
          profile=False,
          test=db2)
 
Index: NeuralNetworks/loadPlot.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import typing as tp\r\nif tp.TYPE_CHECKING:\r\n    from . import _\r\n    from ..Topologies import _\r\n    from ..Utils import _\r\n\r\nimport dill as dl\r\n\r\nfrom Utils import AbstractLoad, Plot\r\nfrom NeuralNetworks.neuralNetwork import AbstractNeuralNetwork\r\n\r\n\r\nclass NeuralNetworkParserError(Exception):\r\n    pass\r\n\r\n\r\nclass LoadNeuralNetwork(AbstractLoad):\r\n    DEFAULT_DIR = AbstractNeuralNetwork.DEFAULT_DIR\r\n    FILE_TYPE = AbstractNeuralNetwork.FILE_TYPE\r\n\r\n    def __new__(cls, file, *args, **kwargs) -> \"AbstractNeuralNetwork\":\r\n        return cls.load(file, *args, **kwargs)\r\n\r\n    @classmethod\r\n    def _read(cls, loadFile, *args, **kwargs) -> \"AbstractNeuralNetwork\":\r\n        if AbstractNeuralNetwork not in (nn := dl.load(loadFile)).__class__.__bases__:\r\n            _err = f'\\nfile \"{loadFile.name}\" is not a NeuralNetworkParser'\r\n            raise NeuralNetworkParserError(_err)\r\n\r\n        return nn\r\n\r\n\r\nclass PlotNeuralNetwork(Plot):\r\n    COST_LABEL = 'Cost'\r\n    ACCURACY_LABEL = 'Accuracy(%)'\r\n    EPOCH_LABEL = 'Epoch'\r\n\r\n    @staticmethod\r\n    def epochGraphs(yh, yLabel, ylim=None):\r\n        xh = []\r\n        index = 0\r\n        [(xh.append([i + index for i in range(len(y))]), index := index + len(y) - 1) for y in yh]\r\n        ax = PlotNeuralNetwork.plotHeight(xh, yh, cluster=True, scatterLabels=[[f\"{yi:.2f}\" for yi in y] for y in yh])\r\n        ax.set_ylabel(yLabel)\r\n        ax.set_xlabel(PlotNeuralNetwork.EPOCH_LABEL)\r\n        if ylim is not None:\r\n            ax.set_ylim(ylim)\r\n\r\n        return ax\r\n\r\n    # plots cost graphs of neural networks\r\n    @staticmethod\r\n    def showCostGraph(nn: \"AbstractNeuralNetwork\") -> \"None\":\r\n        yh = nn.costHistory\r\n        yh[0][0] = yh[0][1]\r\n        PlotNeuralNetwork.epochGraphs(yh, PlotNeuralNetwork.COST_LABEL)\r\n        PlotNeuralNetwork.show()\r\n\r\n    @staticmethod\r\n    def showAccuracyGraph(nn: \"AbstractNeuralNetwork\") -> \"None\":\r\n        yh = nn.accuracyHistory\r\n        PlotNeuralNetwork.epochGraphs(yh, PlotNeuralNetwork.ACCURACY_LABEL, (0, 100))\r\n        PlotNeuralNetwork.show()\r\n\r\n    @staticmethod\r\n    def showCostVsAccuracyGraph(nn):\r\n        yh = nn.accuracyHistory\r\n        xh = nn.costHistory\r\n        xh[0][0] = xh[0][1]\r\n        ax = PlotNeuralNetwork.plotHeight(xh, yh, cluster=True, scatterRotation=60,\r\n                                          scatterLabels=[[f\"({xi:.2f},{yi:.2f})\" for yi, xi in zip(x, y)]\r\n                                                         for x, y in zip(xh, yh)])\r\n        ax.invert_xaxis()\r\n        ax.set_ylim(0, 100)\r\n        ax.set_ylabel(PlotNeuralNetwork.ACCURACY_LABEL)\r\n        ax.set_xlabel(PlotNeuralNetwork.COST_LABEL)\r\n        PlotNeuralNetwork.show()\r\n
===================================================================
diff --git a/NeuralNetworks/loadPlot.py b/NeuralNetworks/loadPlot.py
--- a/NeuralNetworks/loadPlot.py	
+++ b/NeuralNetworks/loadPlot.py	
@@ -4,7 +4,7 @@
     from ..Topologies import _
     from ..Utils import _
 
-import dill as dl
+# import dill as dl
 
 from Utils import AbstractLoad, Plot
 from NeuralNetworks.neuralNetwork import AbstractNeuralNetwork
@@ -23,11 +23,12 @@
 
     @classmethod
     def _read(cls, loadFile, *args, **kwargs) -> "AbstractNeuralNetwork":
-        if AbstractNeuralNetwork not in (nn := dl.load(loadFile)).__class__.__bases__:
-            _err = f'\nfile "{loadFile.name}" is not a NeuralNetworkParser'
-            raise NeuralNetworkParserError(_err)
-
-        return nn
+        # if AbstractNeuralNetwork not in (nn := dl.load(loadFile)).__class__.__bases__:
+        #     _err = f'\nfile "{loadFile.name}" is not a NeuralNetworkParser'
+        #     raise NeuralNetworkParserError(_err)
+        #
+        # return nn
+        pass
 
 
 class PlotNeuralNetwork(Plot):
Index: NeuralNetworks/neuralNetwork.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import typing as tp\r\nif tp.TYPE_CHECKING:\r\n    from . import _\r\n    from ..Topologies import DataBase, LossFunction\r\n    from ..Utils import Shape, Activators\r\nimport cProfile as cP\r\nimport time as tm\r\nimport warnings as wr\r\nfrom abc import ABCMeta, abstractmethod\r\n\r\nimport dill as dl\r\nimport numpy as np\r\n\r\nfrom ._printVars import PrintVars as pV\r\nfrom Utils import AbstractSave\r\n\r\n\r\n# todo: implement auto save. L\r\nclass AbstractNeuralNetwork(AbstractSave, metaclass=ABCMeta):\r\n    DEFAULT_DIR = '\\\\Models\\\\'\r\n    DEFAULT_NAME = 'nn'\r\n    FILE_TYPE = '.nnt'\r\n    SMOOTH_PRINT_INTERVAL = 0.25\r\n\r\n    def saveName(self) -> str:\r\n        return f\"{int(self.costTrained * 100)}c.{self.epochTrained}e.{self.secToHMS(self.timeTrained)}\"\r\n\r\n    def _write(self, dumpFile, *args, **kwargs):\r\n        self._initializeVars()\r\n        trainDataBase = self.trainDataBase\r\n        self.trainDataBase = None\r\n        dl.dump(self, dumpFile)\r\n        self.trainDataBase: \"DataBase\" = trainDataBase\r\n\r\n    def __init__(self, shape: \"Shape\", activators: \"Activators\", costFunction: \"LossFunction\"):\r\n        self.shape = shape\r\n        self.activations, self.activationDerivatives = activators(self.shape.LAYERS - 1)\r\n        self.lossFunction = costFunction\r\n\r\n        self.costHistory = []\r\n        self.accuracyHistory = []\r\n        self.costTrained = 0\r\n        self.timeTrained = 0\r\n        self.epochTrained = 0\r\n        self.trainAccuracy = float('nan')\r\n        self.testAccuracy = float('nan')\r\n\r\n        self.epochs = 1\r\n        self.batchSize = 32\r\n        self.numBatches = None\r\n        self.training = False\r\n        self.profiling = False\r\n        self.__neverTrained = True\r\n\r\n        self.outputs = None\r\n        self.target = None\r\n        self.deltaLoss = None\r\n        self.loss = None\r\n\r\n        self.trainDataBase = None\r\n        self.optimizer = None\r\n\r\n    @abstractmethod\r\n    def _forwardPass(self, layer=1):\r\n        pass\r\n\r\n    @abstractmethod\r\n    def _backPropagate(self, layer=-1):\r\n        pass\r\n\r\n    @abstractmethod\r\n    def _fire(self, layer):\r\n        pass\r\n\r\n    @abstractmethod\r\n    def _wire(self, layer):\r\n        pass\r\n\r\n    @abstractmethod\r\n    def _initializeVars(self):\r\n        pass\r\n\r\n    @abstractmethod\r\n    def _trainer(self):\r\n        \"\"\"assign self.loss and self.deltaLoss here\"\"\"\r\n\r\n    def process(self, inputs) -> \"np.ndarray\":\r\n        if self.training:\r\n            wr.showwarning(\"processing while training in progress, may have unintended conflicts\", ResourceWarning,\r\n                           'neuralNetwork.py->AbstractNeuralNetwork.process', 0)\r\n            return np.NAN\r\n\r\n        inputs = np.array(inputs).reshape([-1, *self.shape.INPUT])\r\n        self.outputs[0] = inputs\r\n        self._forwardPass()\r\n        rVal = self.outputs[-1]\r\n        self._initializeVars()\r\n\r\n        return rVal\r\n\r\n    @staticmethod\r\n    def _statPrinter(key, value, prefix='', suffix=pV.CEND, end=' '):\r\n        print(prefix + f\"{key}:{value}\" + suffix, end=end)\r\n\r\n    def train(self, epochs: \"int\",\r\n              batchSize: \"int\",\r\n              trainDataBase: \"DataBase\",\r\n              optimizer,\r\n              profile=False,\r\n              test=None):\r\n        if epochs is not None:\r\n            self.epochs = epochs\r\n        if batchSize is not None:\r\n            self.batchSize = batchSize\r\n        if trainDataBase is not None:\r\n            self.trainDataBase = trainDataBase\r\n        if optimizer is not None:\r\n            self.optimizer = optimizer\r\n\r\n        self.deltaLoss = [(np.zeros((self.batchSize, *self.shape[i]), dtype=np.float32))\r\n                          for i in range(0, self.shape.LAYERS)]\r\n\r\n        if not profile:\r\n            if self.__neverTrained:\r\n                self.outputs[0], self.target = self.trainDataBase.batchGenerator(self.batchSize).send(-1)\r\n                self._forwardPass(); self._trainer(); self._backPropagate()\r\n                trainCosts, trainAccuracy = [self.loss], [self._tester(self.outputs[-1], self.target)]\r\n                self._initializeVars()\r\n                self.__neverTrained = False\r\n            else: trainCosts, trainAccuracy = [self.costHistory[-1][-1]], [self.accuracyHistory[-1][-1]]\r\n            self.training = True\r\n            self.numBatches = int(np.ceil(self.trainDataBase.size / self.batchSize))\r\n            trainTime = 0\r\n            nextPrintTime = self.SMOOTH_PRINT_INTERVAL\r\n            self._statPrinter('Epoch', f\"0/{self.epochs}\", prefix=pV.CBOLDITALICURL + pV.CBLUE)\r\n            for epoch in range(1, self.epochs + 1):\r\n                self.costTrained = self.trainAccuracy = 0\r\n                time = tm.time()\r\n                batchGenerator = self.trainDataBase.batchGenerator(self.batchSize)\r\n                for batch in range(self.numBatches):\r\n                    self.outputs[0], self.target = batchGenerator.__next__()\r\n                    self._forwardPass(); self._trainer(); self._backPropagate()\r\n                    self.costTrained += self.loss; self.trainAccuracy += self._tester(self.outputs[-1], self.target)\r\n                epochTime = tm.time() - time; trainTime += epochTime\r\n                self.costTrained /= self.trainDataBase.size; self.trainAccuracy /= self.numBatches\r\n                trainCosts.append(self.costTrained); trainAccuracy.append(self.trainAccuracy)\r\n                if trainTime >= nextPrintTime or epoch == self.epochs:\r\n                    nextPrintTime += self.SMOOTH_PRINT_INTERVAL\r\n                    avgTime = trainTime / epoch\r\n                    print(end='\\r')\r\n                    self._statPrinter('Epoch', f\"{epoch}/{self.epochs}\", prefix=pV.CBOLDITALICURL + pV.CBLUE)\r\n                    self._statPrinter('Cost', f\"{self.costTrained:.8f}\", prefix=pV.CYELLOW, suffix='')\r\n                    self._statPrinter('Cost-Reduction', f\"{(trainCosts[-2] - self.costTrained):.8f}\")\r\n                    self._statPrinter('Time', self.secToHMS(epochTime), prefix=pV.CBOLD + pV.CRED2, suffix='')\r\n                    self._statPrinter('Average-Time', self.secToHMS(avgTime), suffix='')\r\n                    self._statPrinter('Eta', self.secToHMS(avgTime * (self.epochs - epoch)), suffix='')\r\n                    self._statPrinter('Elapsed', self.secToHMS(trainTime))\r\n                self.epochTrained += 1\r\n            print()\r\n            self.timeTrained += trainTime\r\n            self.costHistory.append(trainCosts); self.accuracyHistory.append(trainAccuracy)\r\n            self.training = False\r\n            self._initializeVars()\r\n        else:\r\n            self.profiling = True; cP.runctx(\"self.train()\", globals=globals(), locals=locals()); self.profiling = False\r\n\r\n        if not self.profiling: self.test(test)\r\n\r\n    @staticmethod\r\n    def secToHMS(seconds, hms=('h', 'm', 's')):\r\n        encode = f'%S{hms[2]}'\r\n        if (tim := tm.gmtime(seconds)).tm_min != 0:\r\n            encode = f'%M{hms[1]}' + encode\r\n        if tim.tm_hour != 0:\r\n            encode = f'%H{hms[0]}' + encode\r\n\r\n        return tm.strftime(encode, tim)\r\n\r\n    @staticmethod\r\n    def _tester(out, tar):\r\n        if np.shape(tar) != 1:\r\n            outIndex = np.where(out == np.max(out, axis=1, keepdims=True))[1]\r\n            targetIndex = np.where(tar == 1)[1]\r\n        else:\r\n            outIndex = np.round(out)\r\n            targetIndex = tar\r\n        result = outIndex == targetIndex\r\n        result = np.float32(1) * result\r\n\r\n        return result.sum() / result.shape[0] * 100\r\n\r\n    def accuracy(self, inputSet, targetSet):\r\n        assert (size := np.shape(inputSet)[0]) == np.shape(targetSet)[0], \\\r\n            \"the size of both inputSet and targetSet should be same\"\r\n        try:\r\n            return self._tester(self.process(inputSet), targetSet)\r\n        except MemoryError:\r\n            accuracy1 = self.accuracy(inputSet[:(to := size // 2)], targetSet[:to])\r\n            accuracy2 = self.accuracy(inputSet[to:], targetSet[to:])\r\n            return (accuracy1 + accuracy2) / 2\r\n\r\n    def test(self, testDataBase: \"DataBase\" = None):\r\n        self._statPrinter('Testing', 'wait...', prefix=pV.CBOLD + pV.CYELLOW, suffix='')\r\n        if self.trainDataBase is not None:\r\n            self.trainAccuracy = self.accuracy(self.trainDataBase.inputSet, self.trainDataBase.targetSet)\r\n        if testDataBase is not None:\r\n            self.testAccuracy = self.accuracy(testDataBase.inputSet, testDataBase.targetSet)\r\n        print(end='\\r')\r\n        self._statPrinter('Train-Accuracy', f\"{self.trainAccuracy}%\", suffix='', end='\\n')\r\n        self._statPrinter('Test-Accuracy', f\"{self.testAccuracy}%\", end='\\n')\r\n        self._initializeVars()\r\n
===================================================================
diff --git a/NeuralNetworks/neuralNetwork.py b/NeuralNetworks/neuralNetwork.py
--- a/NeuralNetworks/neuralNetwork.py	
+++ b/NeuralNetworks/neuralNetwork.py	
@@ -8,7 +8,7 @@
 import warnings as wr
 from abc import ABCMeta, abstractmethod
 
-import dill as dl
+# import dill as dl
 import numpy as np
 
 from ._printVars import PrintVars as pV
@@ -29,7 +29,7 @@
         self._initializeVars()
         trainDataBase = self.trainDataBase
         self.trainDataBase = None
-        dl.dump(self, dumpFile)
+        # dl.dump(self, dumpFile)
         self.trainDataBase: "DataBase" = trainDataBase
 
     def __init__(self, shape: "Shape", activators: "Activators", costFunction: "LossFunction"):
