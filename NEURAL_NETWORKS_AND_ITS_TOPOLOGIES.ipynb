{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "original_NEURAL_NETWORKS_AND_ITS_TOPOLOGIES.ipynb",
   "provenance": [
    {
     "file_id": "1wRQs_HafqEJaUdcgK5h0iOpDS8L5R6Jw",
     "timestamp": 1621847501296
    }
   ],
   "collapsed_sections": [
    "k-MT-43t_aX2",
    "cAr7VsJuEDB8",
    "4D_eNY6qD3sv",
    "_ySxDDOlXhKL",
    "gLIOMp4_nERt"
   ],
   "mount_file_id": "1wRQs_HafqEJaUdcgK5h0iOpDS8L5R6Jw",
   "authorship_tag": "ABX9TyMa1YGkvn5BQjHNTVx8mG+a"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "id": "qRHV-a8oK00L"
   },
   "source": [
    "#@title Mount Gdrive to Access training data from Gdrive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(‘/content/drive’)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-MT-43t_aX2"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JBt8ZB3KBNYp"
   },
   "source": [
    "import cProfile as cp\n",
    "import ctypes\n",
    "import os\n",
    "import time as tm\n",
    "import warnings\n",
    "import gzip\n",
    "from typing import *\n",
    "\n",
    "import dill\n",
    "import numpy as np\n",
    "from matplotlib import collections as mc, pyplot as plt\n",
    "\n",
    "np.NONE = [np.array([None])]\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "CEND = '\\33[0m'\n",
    "CBOLD = '\\33[1m'\n",
    "CITALIC = '\\33[3m'\n",
    "CURL = '\\33[4m'\n",
    "CBLINK = '\\33[5m'\n",
    "CBLINK2 = '\\33[6m'\n",
    "CSELECTED = '\\33[7m'\n",
    "\n",
    "CBLACK = '\\33[30m'\n",
    "CRED = '\\33[31m'\n",
    "CGREEN = '\\33[32m'\n",
    "CYELLOW = '\\33[33m'\n",
    "CBLUE = '\\33[34m'\n",
    "CVIOLET = '\\33[35m'\n",
    "CBEIGE = '\\33[36m'\n",
    "CWHITE = '\\33[37m'\n",
    "\n",
    "CBLACKBG = '\\33[40m'\n",
    "CREDBG = '\\33[41m'\n",
    "CGREENBG = '\\33[42m'\n",
    "CYELLOWBG = '\\33[43m'\n",
    "CBLUEBG = '\\33[44m'\n",
    "CVIOLETBG = '\\33[45m'\n",
    "CBEIGEBG = '\\33[46m'\n",
    "CWHITEBG = '\\33[47m'\n",
    "\n",
    "CGREY = '\\33[90m'\n",
    "CRED2 = '\\33[91m'\n",
    "CGREEN2 = '\\33[92m'\n",
    "CYELLOW2 = '\\33[93m'\n",
    "CBLUE2 = '\\33[94m'\n",
    "CVIOLET2 = '\\33[95m'\n",
    "CBEIGE2 = '\\33[96m'\n",
    "CWHITE2 = '\\33[97m'\n",
    "\n",
    "CGREYBG = '\\33[100m'\n",
    "CREDBG2 = '\\33[101m'\n",
    "CGREENBG2 = '\\33[102m'\n",
    "CYELLOWBG2 = '\\33[103m'\n",
    "CBLUEBG2 = '\\33[104m'\n",
    "CVIOLETBG2 = '\\33[105m'\n",
    "CBEIGEBG2 = '\\33[106m'\n",
    "CWHITEBG2 = '\\33[107m'\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAr7VsJuEDB8"
   },
   "source": [
    "# Topologies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r1yDUaZ05v7b"
   },
   "source": [
    "class Initializer:\n",
    "    # for custom initializer\n",
    "    def __init__(self, initializer, *args, **kwargs):\n",
    "        self.initialize = initializer\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def initialize(self, shape, layers):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def uniform(start=-1, stop=1):\n",
    "        def initializer(shape, layers):\n",
    "            biases = [np.random.uniform(start, stop, (shape[i], 1)).astype(dtype=np.float32)\n",
    "                      for i in range(1, layers)]\n",
    "            weights = [np.random.uniform(start, stop, (shape[i], shape[i - 1])).astype(dtype=np.float32)\n",
    "                       for i in range(1, layers)]\n",
    "\n",
    "            return np.NONE + biases, np.NONE + weights\n",
    "\n",
    "        return Initializer(initializer)\n",
    "\n",
    "    @staticmethod\n",
    "    def normal(scale=1):\n",
    "        def initializer(shape, layers):\n",
    "            biases = [(np.random.default_rng().standard_normal((shape[i], 1), dtype=np.float32)) * scale\n",
    "                      for i in range(1, layers)]\n",
    "            weights = [(np.random.default_rng().standard_normal((shape[i], shape[i - 1]), dtype=np.float32)) * scale\n",
    "                       for i in range(1, layers)]\n",
    "\n",
    "            return np.NONE + biases, np.NONE + weights\n",
    "\n",
    "        return Initializer(initializer)\n",
    "\n",
    "    @staticmethod\n",
    "    def xavier(he=1):\n",
    "        def initializer(shape, layers):\n",
    "            biases = [np.random.default_rng().standard_normal((shape[i], 1),\n",
    "                                                              dtype=np.float32) * (he / shape[i - 1]) ** 0.5\n",
    "                      for i in range(1, layers)]\n",
    "            weights = [np.random.default_rng().standard_normal((shape[i], shape[i - 1]),\n",
    "                                                               dtype=np.float32) * (he / shape[i - 1]) ** 0.5\n",
    "                       for i in range(1, layers)]\n",
    "\n",
    "            return np.NONE + biases, np.NONE + weights\n",
    "\n",
    "        return Initializer(initializer)\n",
    "\n",
    "    @staticmethod\n",
    "    def normalized_xavier(he=6):\n",
    "        def initializer(shape, layers):\n",
    "            biases = [np.random.default_rng().standard_normal((shape[i], 1), dtype=np.float32) *\n",
    "                      (he / (shape[i - 1] + shape[i])) ** 0.5\n",
    "                      for i in range(1, layers)]\n",
    "            weights = [np.random.default_rng().standard_normal((shape[i], shape[i - 1]), dtype=np.float32) *\n",
    "                       (he / (shape[i - 1] + shape[i])) ** 0.5\n",
    "                       for i in range(1, layers)]\n",
    "\n",
    "            return np.NONE + biases, np.NONE + weights\n",
    "\n",
    "        return Initializer(initializer)\n",
    "\n",
    "\n",
    "class ActivationFunction:\n",
    "    # for custom activation_function\n",
    "    def __init__(self, activation, activated_derivative, *args, **kwargs):\n",
    "        self.activations = activation, activated_derivative\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    activations = None\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(smooth=1, offset=0):\n",
    "        ONE = np.float32(1)\n",
    "        E = np.float32(np.e)\n",
    "        SMOOTH = np.float32(smooth)\n",
    "        OFFSET = np.float32(offset)\n",
    "\n",
    "        def activation(x):\n",
    "            return ONE / (ONE + E ** (-SMOOTH * (x + OFFSET)))\n",
    "\n",
    "        def activated_derivative(activated_x):\n",
    "            return SMOOTH * (activated_x * (ONE - activated_x))\n",
    "\n",
    "        return ActivationFunction(activation, activated_derivative)\n",
    "\n",
    "    @staticmethod\n",
    "    def tanh(alpha=1):\n",
    "        ALPHA = np.float32(alpha)\n",
    "\n",
    "        def activation(x):\n",
    "            return np.arctan(ALPHA * x)\n",
    "\n",
    "        def activated_derivative(activated_x):\n",
    "            return ALPHA * np.square(np.cos(activated_x))\n",
    "\n",
    "        return ActivationFunction(activation, activated_derivative)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu():\n",
    "        ONE = np.float32(1)\n",
    "\n",
    "        def activation(x):\n",
    "            return x * (x > 0)\n",
    "\n",
    "        def activated_derivative(activated_x):\n",
    "            return ONE * (activated_x != 0)\n",
    "\n",
    "        return ActivationFunction(activation, activated_derivative)\n",
    "\n",
    "    @staticmethod\n",
    "    def prelu(leak=0.1):\n",
    "        ONE = np.float32(1)\n",
    "        LEAK = np.float32(leak)\n",
    "\n",
    "        def activation(x):\n",
    "            return np.where(x > 0, x, LEAK * x)\n",
    "\n",
    "        def activated_derivative(activated_x):\n",
    "            return np.where(activated_x == 0, LEAK, ONE)\n",
    "\n",
    "        return ActivationFunction(activation, activated_derivative)\n",
    "\n",
    "    @staticmethod\n",
    "    def elu(alpha=1):\n",
    "        ONE = np.float32(1)\n",
    "        E = np.e\n",
    "        ALPHA = np.float32(alpha)\n",
    "\n",
    "        def activation(x):\n",
    "            return np.where(x > 0, x, ALPHA * (E ** x - 1))\n",
    "\n",
    "        def activated_derivative(activated_x):\n",
    "            return np.where(activated_x != 0, ONE, activated_x + ALPHA)\n",
    "\n",
    "        return ActivationFunction(activation, activated_derivative)\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax():\n",
    "        E = np.float32(np.e)\n",
    "\n",
    "        def activation(x):\n",
    "            numerator = E ** (x - x.max(axis=1)[:, None])\n",
    "\n",
    "            return numerator / np.einsum('lij->lj', numerator)[:, None]\n",
    "\n",
    "        def activated_derivative(activated_x):\n",
    "            jacobian = -np.einsum('lij,lkj->lik', activated_x, activated_x)\n",
    "            diag_i = np.diag_indices(jacobian.shape[1])\n",
    "            jacobian[:, diag_i[1], diag_i[0]] = np.einsum('lij,lij->li', activated_x, 1 - activated_x)\n",
    "\n",
    "            return jacobian\n",
    "\n",
    "        return ActivationFunction(activation, activated_derivative)\n",
    "\n",
    "    @staticmethod\n",
    "    def softplus():\n",
    "        E = np.float32(np.e)\n",
    "        ONE = np.float32(1)\n",
    "\n",
    "        def activation(x):\n",
    "            return np.log(ONE + E ** x)\n",
    "\n",
    "        def activated_derivative(activated_x):\n",
    "            return ONE - E ** -activated_x\n",
    "\n",
    "        return ActivationFunction(activation, activated_derivative)\n",
    "\n",
    "\n",
    "class LossFunction:\n",
    "    # for custom loss_function\n",
    "    def __init__(self, loss_function, *args, **kwargs):\n",
    "        self.loss_function = loss_function\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def loss_function(self, output, target):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_square():\n",
    "        def loss_function(output, target):\n",
    "            loss = output - target\n",
    "\n",
    "            return np.einsum('lij,lij->', loss, loss), loss\n",
    "\n",
    "        return LossFunction(loss_function)\n",
    "\n",
    "    # doesn't work\n",
    "    @staticmethod\n",
    "    def cross_entropy():\n",
    "        def loss_function(output, target):\n",
    "            return -np.einsum('lij,lij->', np.log(output), target), -target / output\n",
    "\n",
    "        return LossFunction(loss_function)\n",
    "\n",
    "\n",
    "class WBOptimizer:\n",
    "    # for custom optimizer\n",
    "    def __init__(self, optimizer, *args, **kwargs):\n",
    "        self.optimize = optimizer\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def optimize(self, layer):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient_decent(this: 'ArtificialNeuralNetwork', learning_rate=0.01):\n",
    "        LEARNING_RATE = np.float32(learning_rate)\n",
    "\n",
    "        def optimizer(layer):\n",
    "            this.delta_biases[layer] *= LEARNING_RATE\n",
    "            this.delta_weights[layer] *= LEARNING_RATE\n",
    "\n",
    "        return WBOptimizer(optimizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def momentum(this: 'ArtificialNeuralNetwork', learning_rate=0.001, alpha=None):\n",
    "        if alpha is None: alpha = learning_rate\n",
    "        LEARNING_RATE = np.float32(learning_rate)\n",
    "        ALPHA = np.float32(alpha)\n",
    "        this.pdb, this.pdw = this.delta_initializer(1)  # pdb -> prev_delta_biases, pdw -> prev_delta_weights\n",
    "\n",
    "        def optimizer(layer):\n",
    "            this.delta_biases[layer] = this.pdb[layer] = ALPHA * this.pdb[layer] + \\\n",
    "                                                         LEARNING_RATE * this.delta_biases[layer]\n",
    "            this.delta_weights[layer] = this.pdw[layer] = ALPHA * this.pdw[layer] + \\\n",
    "                                                          LEARNING_RATE * this.delta_weights[layer]\n",
    "\n",
    "        return WBOptimizer(optimizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def decay(this: 'ArtificialNeuralNetwork', learning_rate=0.01, alpha=None):\n",
    "        if alpha is None: alpha = 1 / learning_rate\n",
    "        LEARNING_RATE = np.float32(learning_rate)\n",
    "        ALPHA = np.float32(alpha)\n",
    "        this.decay_count = 0\n",
    "\n",
    "        def optimizer(layer):\n",
    "            k = LEARNING_RATE / (1 + this.decay_count / ALPHA)\n",
    "            this.delta_biases[layer] *= k\n",
    "            this.delta_weights[layer] *= k\n",
    "\n",
    "            this.decay_count += 1 / this.batches\n",
    "\n",
    "        return WBOptimizer(optimizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def nesterov(this: 'ArtificialNeuralNetwork', learning_rate=0.001, alpha=None):\n",
    "        if alpha is None: alpha = learning_rate\n",
    "        LEARNING_RATE = np.float32(learning_rate)\n",
    "        ALPHA = np.float32(alpha)\n",
    "        this.pdb, this.pdw = this.delta_initializer(1)  # pdb -> prev_delta_biases, pdw -> prev_delta_weights\n",
    "\n",
    "        def optimizer(layer):\n",
    "            this.theta[layer] = this.weights[layer] - ALPHA * this.pdw[layer]\n",
    "            this.delta_biases[layer] = this.pdb[layer] = ALPHA * this.pdb[layer] + \\\n",
    "                                                         LEARNING_RATE * this.delta_biases[layer]\n",
    "            this.delta_weights[layer] = this.pdw[layer] = ALPHA * this.pdw[layer] + \\\n",
    "                                                          LEARNING_RATE * this.delta_weights[layer]\n",
    "\n",
    "        return WBOptimizer(optimizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def adagrad(this: 'ArtificialNeuralNetwork', learning_rate=0.01, epsilon=np.e ** -8):\n",
    "        LEARNING_RATE = np.float32(learning_rate)\n",
    "        EPSILON = np.float32(epsilon)\n",
    "        this.initialize = True\n",
    "        this.gsq_b, this.gsq_w = this.delta_initializer(1)  # gsq_b -> grad_square_biases, gsq_w -> grad_square_weights\n",
    "\n",
    "        def optimizer(layer):\n",
    "            if this.initialize:\n",
    "                this.gsq_b, this.gsq_w = this.delta_initializer()\n",
    "                this.initialize = False\n",
    "\n",
    "            this.gsq_b[layer] += np.einsum('lij,lij->lij', this.delta_biases[layer], this.delta_biases[layer])\n",
    "            this.gsq_w[layer] += np.einsum('ij,ij->ij', this.delta_weights[layer], this.delta_weights[layer])\n",
    "\n",
    "            this.delta_biases[layer] *= LEARNING_RATE / np.sqrt(this.gsq_b[layer] + EPSILON)\n",
    "            this.delta_weights[layer] *= LEARNING_RATE / np.sqrt(this.gsq_w[layer] + EPSILON)\n",
    "\n",
    "        return WBOptimizer(optimizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def rmsprop(this: 'ArtificialNeuralNetwork', learning_rate=0.001, beta=0.95, epsilon=np.e ** -8):\n",
    "        LEARNING_RATE = np.float32(learning_rate)\n",
    "        EPSILON = np.float32(epsilon)\n",
    "        BETA = np.float32(beta)\n",
    "        BETA_BAR = np.float32(1 - beta)\n",
    "        this.initialize = True\n",
    "        this.gsq_b, this.gsq_w = this.delta_initializer(1)  # gsq_b -> grad_square_biases, gsq_w -> grad_square_weights\n",
    "\n",
    "        def optimizer(layer):\n",
    "            if this.initialize:\n",
    "                this.gsq_b, this.gsq_w = this.delta_initializer()\n",
    "                this.initialize = False\n",
    "\n",
    "            this.gsq_b[layer] = BETA * this.gsq_b[layer] + \\\n",
    "                                BETA_BAR * np.einsum('lij,lij->lij', this.delta_biases[layer], this.delta_biases[layer])\n",
    "            this.gsq_w[layer] = BETA * this.gsq_w[layer] + \\\n",
    "                                BETA_BAR * np.einsum('ij,ij->ij', this.delta_weights[layer], this.delta_weights[layer])\n",
    "\n",
    "            this.delta_biases[layer] *= LEARNING_RATE / np.sqrt(this.gsq_b[layer] + EPSILON)\n",
    "            this.delta_weights[layer] *= LEARNING_RATE / np.sqrt(this.gsq_w[layer] + EPSILON)\n",
    "\n",
    "        return WBOptimizer(optimizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def adadelta(this: 'ArtificialNeuralNetwork', learning_rate=0.1, alpha=0.95, epsilon=np.e ** -8):\n",
    "        LEARNING_RATE = np.float32(learning_rate)\n",
    "        ALPHA = np.float32(alpha)\n",
    "        ALPHA_BAR = np.float32(1 - alpha)\n",
    "        EPSILON = np.float32(epsilon)\n",
    "        this.initialize = True\n",
    "        this.gsq_b, this.gsq_w = this.delta_initializer(1)  # gsq_b -> grad_square_biases, gsq_w -> grad_square_weights\n",
    "        # dsq_b -> delta_square_biases, dsq_w -> delta_square_weights\n",
    "        this.dsq_b, this.dsq_w = this.delta_initializer(1)\n",
    "\n",
    "        def optimizer(layer):\n",
    "            if this.initialize:\n",
    "                this.gsq_b, this.gsq_w = this.delta_initializer()\n",
    "                this.dsq_b, this.dsq_w = this.delta_initializer()\n",
    "                this.initialize = False\n",
    "\n",
    "            this.gsq_b[layer] = ALPHA * this.gsq_b[layer] + \\\n",
    "                                ALPHA_BAR * np.einsum('lij,lij->lij', this.delta_biases[layer],\n",
    "                                                      this.delta_biases[layer])\n",
    "            this.gsq_w[layer] = ALPHA * this.gsq_w[layer] + \\\n",
    "                                ALPHA_BAR * np.einsum('ij,ij->ij', this.delta_weights[layer], this.delta_weights[layer])\n",
    "\n",
    "            this.delta_biases[layer] *= LEARNING_RATE * \\\n",
    "                                        np.sqrt((this.dsq_b[layer] + EPSILON) / (this.gsq_b[layer] + EPSILON))\n",
    "            this.delta_weights[layer] *= LEARNING_RATE * \\\n",
    "                                         np.sqrt((this.dsq_w[layer] + EPSILON) / (this.gsq_w[layer] + EPSILON))\n",
    "\n",
    "            this.dsq_b[layer] = ALPHA * this.dsq_b[layer] + \\\n",
    "                                ALPHA_BAR * np.einsum('lij,lij->lij', this.delta_biases[layer],\n",
    "                                                      this.delta_biases[layer])\n",
    "            this.dsq_w[layer] = ALPHA * this.dsq_w[layer] + \\\n",
    "                                ALPHA_BAR * np.einsum('ij,ij->ij', this.delta_weights[layer], this.delta_weights[layer])\n",
    "\n",
    "        return WBOptimizer(optimizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def adam(this: 'ArtificialNeuralNetwork', learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=np.e ** -8):\n",
    "        LEARNING_RATE = np.float32(learning_rate)\n",
    "        BETA1 = np.float32(beta1)\n",
    "        BETA1_BAR = np.float32(1 - beta1)\n",
    "        BETA2 = np.float32(beta2)\n",
    "        BETA2_BAR = np.float32(1 - beta2)\n",
    "        EPSILON = np.float32(epsilon)\n",
    "        this.initialize = True\n",
    "        this.gsq_b, this.gsq_w = this.delta_initializer(1)  # gsq_b -> grad_square_biases, gsq_w -> grad_square_weights\n",
    "        this.gb, this.gw = this.delta_initializer(1)  # gb -> grad_biases, gw -> grad_weights\n",
    "\n",
    "        def optimizer(layer):\n",
    "            if this.initialize:\n",
    "                this.gsq_b, this.gsq_w = this.delta_initializer()\n",
    "                this.gb, this.gw = this.delta_initializer()\n",
    "                this.initialize = False\n",
    "\n",
    "            this.gb[layer] = BETA1 * this.gb[layer] + BETA1_BAR * this.delta_biases[layer]\n",
    "            this.gw[layer] = BETA1 * this.gw[layer] + BETA1_BAR * this.delta_weights[layer]\n",
    "            this.gsq_b[layer] = BETA2 * this.gsq_b[layer] + \\\n",
    "                                BETA2_BAR * np.einsum('lij,lij->lij', this.delta_biases[layer],\n",
    "                                                      this.delta_biases[layer])\n",
    "            this.gsq_w[layer] = BETA2 * this.gsq_w[layer] + \\\n",
    "                                BETA2_BAR * np.einsum('ij,ij->ij', this.delta_weights[layer], this.delta_weights[layer])\n",
    "\n",
    "            div_1 = (1 - BETA1 ** (this.epoch + 1))\n",
    "            div_2 = (1 - BETA2 ** (this.epoch + 1))\n",
    "            gb_sq = this.gsq_b[layer] / div_2\n",
    "            gw_sq = this.gsq_w[layer] / div_2\n",
    "\n",
    "            this.delta_biases[layer] = LEARNING_RATE * this.gb[layer] / div_1 / np.sqrt(gb_sq + EPSILON)\n",
    "            this.delta_weights[layer] = LEARNING_RATE * this.gw[layer] / div_1 / np.sqrt(gw_sq + EPSILON)\n",
    "\n",
    "        return WBOptimizer(optimizer)\n",
    "\n",
    "    # doesn't work\n",
    "    @staticmethod\n",
    "    def adamax(this: 'ArtificialNeuralNetwork', learning_rate=0.0001, beta1=0.9, beta2=0.999):\n",
    "        LEARNING_RATE = np.float32(learning_rate)\n",
    "        BETA1 = np.float32(beta1)\n",
    "        BETA1_BAR = np.float32(1 - beta1)\n",
    "        BETA2 = np.float32(beta2)\n",
    "        this.decay_count = 0\n",
    "        this.initialize = True\n",
    "        this.nb, this.nw = this.delta_initializer(1)  # nb -> _biases, nw -> _weights\n",
    "        this.gb, this.gw = this.delta_initializer(1)  # gb -> grad_biases, gw -> grad_weights\n",
    "\n",
    "        def optimizer(layer):\n",
    "            if this.initialize:\n",
    "                this.nb, this.nw = this.delta_initializer()\n",
    "                this.gb, this.gw = this.delta_initializer()\n",
    "                this.initialize = False\n",
    "            this.gb[layer] = BETA1 * this.gb[layer] + BETA1_BAR * this.delta_biases[layer]\n",
    "            this.gw[layer] = BETA1 * this.gw[layer] + BETA1_BAR * this.delta_weights[layer]\n",
    "\n",
    "            this.nb[layer] = np.maximum(BETA2 * this.nb[layer], np.absolute(this.delta_biases[layer]))\n",
    "            this.nw[layer] = np.maximum(BETA2 * this.nw[layer], np.absolute(this.delta_weights[layer]))\n",
    "\n",
    "            div = (1 - BETA1 ** (this.epoch + 1))\n",
    "            vb = this.gb[layer] / div\n",
    "            vw = this.gw[layer] / div\n",
    "\n",
    "            this.delta_biases[layer] = LEARNING_RATE * vb / this.nb[layer]\n",
    "            this.delta_weights[layer] = LEARNING_RATE * vw / this.nw[layer]\n",
    "\n",
    "        return WBOptimizer(optimizer)\n",
    "\n",
    "\n",
    "# database class for training / testing NN\n",
    "class DataBase:\n",
    "    def __init__(self, input_set, output_set):\n",
    "        # class params declaration\n",
    "        self.input_set = np.array(input_set, dtype=np.float32)\n",
    "        self.output_set = np.array(output_set, dtype=np.float32)\n",
    "\n",
    "        # prevent conflicting sizes of input_set and output_set\n",
    "        size = len(self.input_set)\n",
    "        if size != len(self.output_set):\n",
    "            raise Exception(\"Both input_set and output_set should be of same size\")\n",
    "\n",
    "        # class vars initialization\n",
    "        self.size = size\n",
    "        self.pointer = 0\n",
    "        self.block = False\n",
    "        self.batch_size = None\n",
    "\n",
    "        # shuffle database\n",
    "        self.randomize()\n",
    "\n",
    "    # scale data values within -1 to +1\n",
    "    def normalize(self):\n",
    "        input_scale = np.max(np.absolute(self.input_set))\n",
    "        output_scale = np.max(np.absolute(self.output_set))\n",
    "        self.input_set /= input_scale\n",
    "        self.output_set /= output_scale\n",
    "\n",
    "        return input_scale, output_scale\n",
    "\n",
    "    # randomly shuffle order of data_sets\n",
    "    def randomize(self):\n",
    "        indices = [i for i in range(self.size)]\n",
    "        np.random.shuffle(indices)\n",
    "        self.input_set = self.input_set[indices]\n",
    "        self.output_set = self.output_set[indices]\n",
    "\n",
    "    # create generator object which yields a set of sequential data with fixed predetermined size everytime its called\n",
    "    def batch_generator(self, batch_size):\n",
    "        if self.block:\n",
    "            raise PermissionError(\n",
    "                \"Access Denied: DataBase currently in use, 'end' previous generator before creating a new one\")\n",
    "        self.block = True\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        def generator():\n",
    "            while 1:\n",
    "                i = self.pointer + batch_size\n",
    "                if i >= self.size:\n",
    "                    i = self.size\n",
    "                    r_val = self.__batch(i)\n",
    "                    self.__return()\n",
    "\n",
    "                    yield r_val\n",
    "                    return\n",
    "                signal = yield self.__batch(i)\n",
    "                if signal == 'end': return self.__return()\n",
    "                self.pointer += batch_size\n",
    "\n",
    "        return generator()\n",
    "\n",
    "    # returns fixed size of dataset from pointer sequentially\n",
    "    def __batch(self, i):\n",
    "        r_val = [self.input_set[self.pointer:i], self.output_set[self.pointer:i]]\n",
    "        filled = i - self.pointer\n",
    "        if filled != self.batch_size:\n",
    "            vacant = self.batch_size - filled\n",
    "            r_val[0] = \\\n",
    "                np.append(r_val[0], self.input_set[:vacant]).reshape([self.batch_size, *self.input_set.shape[1:]])\n",
    "            r_val[1] = \\\n",
    "                np.append(r_val[1], self.output_set[:vacant]).reshape([self.batch_size, *self.output_set.shape[1:]])\n",
    "\n",
    "        return r_val\n",
    "\n",
    "    # reinitialize class vars after end of generator\n",
    "    def __return(self):\n",
    "        self.pointer = 0\n",
    "        self.randomize()\n",
    "        self.block = False\n",
    "        self.batch_size = None"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4D_eNY6qD3sv"
   },
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IJklryG45DTu"
   },
   "source": [
    "# ANN class\n",
    "class ArtificialNeuralNetwork:\n",
    "    def __init__(self, shape: Tuple[int, ...],\n",
    "                 initializer: Initializer = None,\n",
    "                 activation_function: ActivationFunction = None,\n",
    "                 output_activation_function: ActivationFunction = None):\n",
    "        # default params\n",
    "        if initializer is None: initializer = Initializer.xavier(2)\n",
    "        if activation_function is None: activation_function = ActivationFunction.elu()\n",
    "        if output_activation_function is None: output_activation_function = ActivationFunction.softmax()\n",
    "\n",
    "        # class params declaration\n",
    "        self.shape = tuple(shape)\n",
    "        self.initializer = initializer\n",
    "        self.activation, self.activated_derivative = activation_function.activations\n",
    "        self.output_activation, self.output_activated_derivative = output_activation_function.activations\n",
    "\n",
    "        # declaration of weights and biases and its relatives\n",
    "        self.layers = len(self.shape)\n",
    "        self.biases, self.weights = self.initializer.initialize(self.shape, self.layers)\n",
    "        self.biases_ones = np.NONE + [np.ones_like(bias, dtype=np.float32) for bias in self.biases[1:]]\n",
    "\n",
    "        # derivation wrt\n",
    "        self.theta = self.weights.copy()\n",
    "\n",
    "        # class vars initialization\n",
    "        self.delta_weights, self.delta_biases = None, None\n",
    "        self.train_database = None\n",
    "        self.epochs: int = 1  # total epochs for current training\n",
    "        self.epoch: int = 0  # current epoch\n",
    "        self.batch_size: int = 32  # fancy format allowed from params, ex: -1\n",
    "        self.bs: int = 0  # actual batch_size\n",
    "        self.batches: int = 0  # total batches for current epoch\n",
    "        self.batch: int = 0  # current batch\n",
    "        self.loss_function: LossFunction = LossFunction.mean_square()\n",
    "        self.optimizer: WBOptimizer = WBOptimizer.adam(self)\n",
    "        self.outputs = None\n",
    "        self.target = None\n",
    "        self.loss = None\n",
    "        self.loss_derivative = None\n",
    "        self.costs: List[List[float]] = []  # accumulation of all costs\n",
    "\n",
    "    # recursive pass\n",
    "    def __forward_pass(self, layer: int = 1):\n",
    "        if layer == self.layers - 1:\n",
    "            self.__fire(layer, self.output_activation)\n",
    "        else:\n",
    "            self.__fire(layer, self.activation)\n",
    "            self.__forward_pass(layer + 1)\n",
    "\n",
    "    # returns output, for online processing\n",
    "    def process(self, inputs, layer: int = 1):\n",
    "        self.outputs[layer - 1] = np.array(inputs, dtype=np.float32)\n",
    "        self.__forward_pass(layer)\n",
    "\n",
    "        return self.outputs[-1]\n",
    "\n",
    "    # neuron fire(activation) at a layer\n",
    "    def __fire(self, layer: int, activation):\n",
    "        self.outputs[layer] = \\\n",
    "            activation(np.einsum('lkj,ik->lij', self.outputs[layer - 1], self.weights[layer]) + self.biases[layer])\n",
    "\n",
    "    # neuron wire(updates to biases and weights) at a layer\n",
    "    def __wire(self, layer: int):\n",
    "        # optimization to sum on column(next line only), 5% time reduction\n",
    "        self.biases[layer] -= (self.delta_biases[layer] * self.biases_ones[layer])[0]\n",
    "        self.weights[layer] -= self.delta_weights[layer]\n",
    "        self.theta = self.weights.copy()\n",
    "\n",
    "    # recursive propagation\n",
    "    def __back_propagation(self, activated_derivative, layer: int = -1):\n",
    "        if layer <= -self.layers: return\n",
    "        np.einsum('lij,lim->lij', self.loss_derivative[layer], activated_derivative(self.outputs[layer]),\n",
    "                  out=self.delta_biases[layer])\n",
    "        np.einsum('lkj,lij->ik', self.outputs[layer - 1], self.delta_biases[layer], out=self.delta_weights[layer])\n",
    "        np.einsum('lij,ik->lkj', self.loss_derivative[layer], self.theta[layer], out=self.loss_derivative[layer - 1])\n",
    "        self.optimizer.optimize(layer)\n",
    "        self.__wire(layer)\n",
    "        self.__back_propagation(self.activated_derivative, layer - 1)\n",
    "\n",
    "    # declaring training params\n",
    "    def trainer(self, train_database: DataBase = None,\n",
    "                loss_function: LossFunction = None,\n",
    "                optimizer: WBOptimizer = None,\n",
    "                epochs: int = None,\n",
    "                batch_size: int = None):\n",
    "        # if new param sent, update existing class var, else use old param\n",
    "        if train_database is not None: self.train_database = train_database\n",
    "        if loss_function is not None: self.loss_function = loss_function\n",
    "        if optimizer is not None: self.optimizer = optimizer\n",
    "        if epochs is not None: self.epochs = epochs\n",
    "        if batch_size is not None: self.batch_size = batch_size\n",
    "\n",
    "        if self.batch_size < 0:\n",
    "            self.bs = self.train_database.size - batch_size - 1\n",
    "        else:\n",
    "            self.bs = self.batch_size\n",
    "\n",
    "        # pre memory allocation for faster training\n",
    "        self.outputs = [np.zeros((self.bs, self.shape[layer], 1), dtype=np.float32) for layer in range(self.layers)]\n",
    "        self.loss_derivative = self.outputs.copy()\n",
    "        self.target = self.outputs[-1].copy()\n",
    "        self.delta_biases, self.delta_weights = self.delta_initializer()\n",
    "\n",
    "    # pre memory allocation and initializer of delta values for wire and optimizer\n",
    "    def delta_initializer(self, bs=None):\n",
    "        if bs is None: bs = self.bs\n",
    "        delta_biases = np.NONE + [(np.zeros((bs, self.shape[i], 1), dtype=np.float32)) for i in range(1, self.layers)]\n",
    "        delta_weights = Initializer.normal(0).initialize(self.shape, self.layers)[1]\n",
    "\n",
    "        return delta_biases, delta_weights\n",
    "\n",
    "    # start training after declaring trainer\n",
    "    def train(self, profile=False):\n",
    "        # if profiling requested run training with cProfile\n",
    "        if not profile:\n",
    "            costs = [0]\n",
    "            tot_time = 0\n",
    "            self.batches = int(np.ceil(self.train_database.size / self.bs))\n",
    "            for self.epoch in range(self.epochs):\n",
    "                batch_generator = self.train_database.batch_generator(self.bs)\n",
    "                cost = 0\n",
    "                time = tm.time()\n",
    "                for self.batch in range(self.batches):\n",
    "                    self.outputs[0], self.target = batch_generator.__next__()\n",
    "                    self.__forward_pass()\n",
    "                    self.loss, self.loss_derivative[-1] = \\\n",
    "                        self.loss_function.loss_function(self.outputs[-1], self.target)\n",
    "                    self.__back_propagation(self.output_activated_derivative)\n",
    "                    cost += self.loss\n",
    "                time = tm.time() - time\n",
    "                cost /= self.train_database.size\n",
    "                costs.append(cost)\n",
    "                tot_time += time\n",
    "                print(end='\\r')\n",
    "                print(CBOLD + CBLUE + CURL + f'epoch:{self.epoch}' + CEND,\n",
    "                      CYELLOW + f'cost:{cost}', f'time:{time}' + CEND,\n",
    "                      CBOLD + f'eta:{tot_time / (self.epoch + 1) * (self.epochs - self.epoch - 1)}',\n",
    "                      CEND, end='')\n",
    "            print()\n",
    "            print(CBOLD + CRED2 + f'tot_time:{tot_time}', f'avg_time:{tot_time / self.epochs}' + CEND)\n",
    "            self.costs.append(costs[1:])\n",
    "        else:\n",
    "            cp.runctx(\"self.train()\", globals=globals(), locals=locals())\n",
    "\n",
    "\n",
    "# NN stats plotting\n",
    "class PlotNeuralNetwork:\n",
    "    @staticmethod\n",
    "    def plot_cost_graph(nn):\n",
    "        costs = []\n",
    "        i = 0\n",
    "        for cost_i in range(len(nn.costs)):\n",
    "            cost = nn.costs[cost_i]\n",
    "            if cost_i > 0: costs.append([costs[-1][-1], (i, cost[0])])\n",
    "            costs.append([(c + i, j) for c, j in enumerate(cost)])\n",
    "            i += len(cost)\n",
    "\n",
    "        lc = mc.LineCollection(costs, colors=['red', 'red', 'green', 'green'], linewidths=1)\n",
    "        sp = plt.subplot()\n",
    "        sp.add_collection(lc)\n",
    "\n",
    "        sp.autoscale()\n",
    "        sp.margins(0.1)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# save NN as dill pickle file(removes database from NN before saving)\n",
    "class SaveNeuralNetwork:\n",
    "    @staticmethod\n",
    "    def save(this, fname=None):\n",
    "        if fname is None: fname = 'nn'\n",
    "        if len(fname) >= 4 and '.nns' == fname[-4:0]: fname.replace('.nns', '')\n",
    "        cost = str(round(this.costs[-1][-1] * 100, 2))\n",
    "        fname += 'c' + cost\n",
    "        train_database = this.train_database\n",
    "        this.train_database = None\n",
    "        fpath = os.getcwd() + '/models/'\n",
    "        spath = fpath + fname\n",
    "        os.makedirs(fpath, exist_ok=True)\n",
    "        dill.dump(this, open(spath + '.nns', 'wb'))\n",
    "        this.train_database = train_database\n",
    "\n",
    "        print(spath)\n",
    "\n",
    "        return spath\n",
    "\n",
    "\n",
    "# load NN as python dill object\n",
    "class LoadNeuralNetwork:\n",
    "    @staticmethod\n",
    "    def load(fname, fpath=None):\n",
    "        if fpath is None:\n",
    "            return dill.load(open(os.getcwd() + '/models/' + fname, 'rb'))\n",
    "        else:\n",
    "            return dill.load(open(fpath, 'rb'))\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ySxDDOlXhKL"
   },
   "source": [
    "# Load from *.gz files"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nElcFOjJXeIY"
   },
   "source": [
    "input_set_file = '*.gz'\n",
    "output_set_file = '*.gz'\n",
    "def training_inputs():\n",
    "    with gzip.open(input_set_file, 'r') as f:\n",
    "        # first 4 bytes is a magic number\n",
    "        magic_number = int.from_bytes(f.read(4), 'big')\n",
    "        # second 4 bytes is the number of images\n",
    "        image_count = int.from_bytes(f.read(4), 'big')\n",
    "        # third 4 bytes is the row count\n",
    "        row_count = int.from_bytes(f.read(4), 'big')\n",
    "        # fourth 4 bytes is the column count\n",
    "        column_count = int.from_bytes(f.read(4), 'big')\n",
    "        # rest is the image pixel data, each pixel is stored as an unsigned byte\n",
    "        # pixel values are 0 to 255\n",
    "        image_data = f.read()\n",
    "        images = np.frombuffer(image_data, dtype=np.uint8).reshape((image_count, row_count * column_count, 1))\n",
    "\n",
    "        return images\n",
    "\n",
    "\n",
    "def training_outputs():\n",
    "    with gzip.open(output_set_file, 'r') as f:\n",
    "        # first 4 bytes is a magic number\n",
    "        magic_number = int.from_bytes(f.read(4), 'big')\n",
    "        # second 4 bytes is the number of labels\n",
    "        label_count = int.from_bytes(f.read(4), 'big')\n",
    "        # rest is the label data, each label is stored as unsigned byte\n",
    "        # label values are 0 to 9\n",
    "        label_data = f.read()\n",
    "        labels = np.frombuffer(label_data, dtype=np.uint8)\n",
    "\n",
    "        return labels\n",
    "\n",
    "\n",
    "input_set, output_set = training_inputs(), training_outputs()\n",
    "\n",
    "# one-hot encoding\n",
    "one_hot_set = np.zeros((output_set.size, output_set.max() + 1, 1))\n",
    "one_hot_set[np.arange(0, output_set.size), output_set] = 1\n",
    "output_set = one_hot_set\n",
    "\n",
    "# uncomment to save at your path\n",
    "# np.savez_compressed('*.nndb', input_set, output_set)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLIOMp4_nERt"
   },
   "source": [
    "# Load from .nndb.npy"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EHy0KE6SnCEH"
   },
   "source": [
    "fpath = '*.nndb.npy'\n",
    "nn_loader = np.load(fpath)\n",
    "input_set, output_set = nn_loader['arr_0'], nn_loader['arr_1']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0U9gUF8nL4MO"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3O2VMEEkScy5"
   },
   "source": [
    "fname = None\n",
    "fpath = None\n",
    "nn = LoadNeuralNetwork.load(fname=fname,\n",
    "                            fpath=fpath)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G9VbxxPdL2uI"
   },
   "source": [
    "# Hyper params\n",
    "shape = ?\n",
    "initializer = None\n",
    "activation_function = None\n",
    "output_activation_function = None\n",
    "\n",
    "# Initialize NN\n",
    "nn = ArtificialNeuralNetwork(shape=shape,\n",
    "                                   initializer=initializer,\n",
    "                                   activation_function=activation_function,\n",
    "                                   output_activation_function=output_activation_function)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TyBp5FipMvEF"
   },
   "source": [
    "# Hyper params\n",
    "input_set = ?\n",
    "output_set = ?\n",
    "train_database = DataBase(input_set, output_set)\n",
    "train_database.normalize()\n",
    "epochs = None\n",
    "batch_size = None\n",
    "loss_function = None\n",
    "optimizer = None\n",
    "\n",
    "# Initialize trainer\n",
    "nn.trainer(train_database=train_database,\n",
    "           epochs=epochs,\n",
    "           batch_size=batch_size,\n",
    "           loss_function=loss_function,\n",
    "           optimizer=optimizer)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4KgpPLwNPkaH"
   },
   "source": [
    "nn.train()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bd-J8PrfUamZ"
   },
   "source": [
    "PlotNeuralNetwork.plot_cost_graph(nn)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "H2vAONXaP95J"
   },
   "source": [
    "SaveNeuralNetwork.save(nn)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}