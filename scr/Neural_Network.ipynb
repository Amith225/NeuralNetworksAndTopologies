{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "alpha_47_class.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11oYn-bTMSI1slOwSKgbv3ETHAg21m7zy",
      "authorship_tag": "ABX9TyOstNDyZ1sbJKl58vKxjp+X",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amith225/NEURAL_NETWORKS_AND_TOPOLOGIES/blob/master/Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoG0cP4ajlea",
        "cellView": "form"
      },
      "source": [
        "#@title Library Imports\r\n",
        "import time as tm\r\n",
        " \r\n",
        "# import cupy as cp\r\n",
        "import numexpr as ne\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import dill as dl\r\n",
        "\r\n",
        "from matplotlib import collections as mc, pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15nTVPSfkDep",
        "cellView": "form"
      },
      "source": [
        "#@title NeuralNetwork And Its Topologies Classes\r\n",
        "class CreateNeuralNetwork:\r\n",
        "    def __init__(self, shape, initializer, activation, output_activation=None):\r\n",
        "        self.shape = shape\r\n",
        "        self.layers = len(self.shape)\r\n",
        "        if output_activation is None: output_activation = activation\r\n",
        "        self.weights, self.biases = initializer(self)\r\n",
        "        self.activation, self.activated_derivative = activation\r\n",
        "        self.output_activation, self.activated_output_derivative = output_activation\r\n",
        "        self.activated_outputs = np.array([np.zeros((self.shape[i], 1), dtype=np.float32) for i in range(self.layers)],\r\n",
        "                                          dtype=np.ndarray)\r\n",
        "        self.delta_weights, self.delta_biases = Initializer.normal(0)(self)\r\n",
        "\r\n",
        "        self.costs = []\r\n",
        "        self.cost = 0\r\n",
        "        self.e = 0\r\n",
        "        self.training_set = None\r\n",
        "        self.epochs = None\r\n",
        "        self.batch_size = None\r\n",
        "        self.loss_function = None\r\n",
        "        self.optimizer = None\r\n",
        "        self.opt = None\r\n",
        "        self.cost_derivative = None\r\n",
        "\r\n",
        "    def process(self, input):\r\n",
        "        self.activated_outputs[0] = np.array(input, dtype=np.float32).reshape((len(input), 1))\r\n",
        "\r\n",
        "        for l in range(self.layers - 2):\r\n",
        "            self.activated_outputs[l + 1] = self.activation(\r\n",
        "                np.einsum('ij,jk->ik', self.weights[l], self.activated_outputs[l],\r\n",
        "                          dtype=np.float32) + self.biases[l])\r\n",
        "\r\n",
        "        return self.activation(np.einsum('ij,jk->ik', self.weights[l], self.activated_outputs[l],\r\n",
        "                                         dtype=np.float32) + self.biases[l])\r\n",
        "\r\n",
        "    def forward_pass(self, input):\r\n",
        "        self.activated_outputs[0] = input\r\n",
        "\r\n",
        "        for l in range(self.layers - 2):\r\n",
        "            self.activated_outputs[l + 1] = self.activation(\r\n",
        "                np.einsum('ij,jk->ik', self.weights[l], self.activated_outputs[l],\r\n",
        "                          dtype=np.float32) + self.biases[l])\r\n",
        "\r\n",
        "        self.activated_outputs[l + 2] = self.output_activation(\r\n",
        "            np.einsum('ij,jk->ik', self.weights[l + 1], self.activated_outputs[l + 1],\r\n",
        "                      dtype=np.float32) + self.biases[l + 1])\r\n",
        "\r\n",
        "    def back_propagation(self, b):\r\n",
        "        self.cost_derivative, cost = self.loss_function(self, b)\r\n",
        "        self.cost += cost\r\n",
        "\r\n",
        "        self.optimizer()\r\n",
        "\r\n",
        "    def find_delta(self, layer, activated_derivative, theta):\r\n",
        "        layer = layer - 1\r\n",
        "        delta_biases = self.cost_derivative * activated_derivative(self.activated_outputs[layer + 1])\r\n",
        "        np.einsum('ij,ji->ij', delta_biases, self.activated_outputs[layer], dtype=np.float32,\r\n",
        "                  out=self.delta_weights[layer])\r\n",
        "\r\n",
        "        self.cost_derivative = np.einsum('ij,ik', theta, self.cost_derivative, dtype=np.float32)\r\n",
        "\r\n",
        "        self.opt(layer)\r\n",
        "        self.weights[layer] -= self.delta_weights[layer]\r\n",
        "        self.biases[layer] -= self.delta_biases[layer]\r\n",
        "\r\n",
        "    def train(self, training_set=None, epochs=None, batch_size=None, loss_function=None, optimizer=None,\r\n",
        "              vectorize=True):\r\n",
        "        if vectorize is True and training_set is not None:\r\n",
        "            training_set = np.array([[np.array(t[0], dtype=np.float32).reshape((len(t[0]), 1)),\r\n",
        "                                      np.array(t[1], dtype=np.float32).reshape((len(t[1]), 1))]\r\n",
        "                                     for t in training_set], dtype=np.ndarray)\r\n",
        "        if training_set is not None: self.training_set = training_set\r\n",
        "        if epochs is not None: self.epochs = epochs\r\n",
        "        if batch_size is not None: self.batch_size = batch_size\r\n",
        "        if loss_function is not None: self.loss_function = loss_function\r\n",
        "        if optimizer is not None: self.optimizer, self.opt = optimizer\r\n",
        "\r\n",
        "        if self.batch_size < 0:\r\n",
        "            batch_size = len(self.training_set) + self.batch_size\r\n",
        "        else:\r\n",
        "            batch_size = self.batch_size\r\n",
        "\r\n",
        "        train_costs = []\r\n",
        "        for e in range(self.epochs):\r\n",
        "            print('epoch:', e, end='  ')\r\n",
        "            batch_set = self.training_set[np.random.choice(self.training_set.shape[0], batch_size, replace=False)]\r\n",
        "\r\n",
        "            t = tm.time()\r\n",
        "            self.cost = 0\r\n",
        "            for b in batch_set:\r\n",
        "                self.back_propagation(b)\r\n",
        "            cost = self.cost / batch_size\r\n",
        "            print('cost:', cost, 'time:', tm.time() - t)\r\n",
        "            train_costs.append(cost)\r\n",
        "\r\n",
        "            self.e += e * batch_size / len(self.training_set)\r\n",
        "        self.costs.append(train_costs)\r\n",
        "\r\n",
        "        self.activated_outputs = np.array([np.zeros((self.shape[i], 1), dtype=np.float32) for i in range(self.layers)],\r\n",
        "                                          dtype=np.ndarray)\r\n",
        "\r\n",
        "    def test(self):\r\n",
        "        pass\r\n",
        "\r\n",
        "\r\n",
        "class Initializer:\r\n",
        "    @staticmethod\r\n",
        "    def uniform(start, stop):\r\n",
        "        def initializer(self):\r\n",
        "            weights = [np.random.uniform(start, stop, (self.shape[i], self.shape[i - 1])).astype(dtype=np.float32)\r\n",
        "                       for i in range(1, self.layers)]\r\n",
        "            biases = [np.random.uniform(start, stop, (self.shape[i], 1)).astype(dtype=np.float32)\r\n",
        "                      for i in range(1, self.layers)]\r\n",
        "\r\n",
        "            return np.array(weights, dtype=np.ndarray), np.array(biases, dtype=np.ndarray)\r\n",
        "\r\n",
        "        return initializer\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def normal(scale=1):\r\n",
        "        def initializer(self):\r\n",
        "            weights = [np.random.default_rng().standard_normal((self.shape[i], self.shape[i - 1]),\r\n",
        "                                                               dtype=np.float32)\r\n",
        "                       for i in range(1, self.layers)]\r\n",
        "            biases = [np.random.default_rng().standard_normal((self.shape[i], 1),\r\n",
        "                                                              dtype=np.float32)\r\n",
        "                      for i in range(1, self.layers)]\r\n",
        "\r\n",
        "            return np.array(weights, dtype=np.ndarray) * scale, np.array(biases, dtype=np.ndarray) * scale\r\n",
        "\r\n",
        "        return initializer\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def xavier(he=1):\r\n",
        "        def initializer(self):\r\n",
        "            weights = [np.random.default_rng().standard_normal((self.shape[i], self.shape[i - 1]),\r\n",
        "                                                               dtype=np.float32) * (he / self.shape[i - 1]) ** 0.5\r\n",
        "                       for i in range(1, self.layers)]\r\n",
        "            biases = [np.random.default_rng().standard_normal((self.shape[i], 1),\r\n",
        "                                                              dtype=np.float32) * (he / self.shape[i - 1]) ** 0.5\r\n",
        "                      for i in range(1, self.layers)]\r\n",
        "\r\n",
        "            return np.array(weights, dtype=np.ndarray), np.array(biases, dtype=np.ndarray)\r\n",
        "\r\n",
        "        return initializer\r\n",
        "\r\n",
        "\r\n",
        "class LossFunction:\r\n",
        "    @staticmethod\r\n",
        "    def mean_square():\r\n",
        "        def loss_function(self, b):\r\n",
        "            self.forward_pass(b[0])\r\n",
        "            cost_derivative = self.activated_outputs[-1] - b[1]\r\n",
        "\r\n",
        "            return cost_derivative, np.einsum('ij,ij->', cost_derivative, cost_derivative, dtype=np.float32)\r\n",
        "\r\n",
        "        return loss_function\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def cross_entropy():\r\n",
        "        def loss_function(self, b):\r\n",
        "            pass\r\n",
        "\r\n",
        "        return loss_function\r\n",
        "\r\n",
        "\r\n",
        "class ActivationFunction:\r\n",
        "    @staticmethod\r\n",
        "    def sigmoid(alpha=1, beta=0):\r\n",
        "        e = np.float32(np.e)\r\n",
        "\r\n",
        "        def activation(x):\r\n",
        "            return 1 / (1 + e ** (-alpha * (x + beta)))\r\n",
        "\r\n",
        "        def activated_derivative(activated_x):\r\n",
        "            return alpha * (activated_x * (1 - activated_x))\r\n",
        "\r\n",
        "        return activation, activated_derivative\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def relu():\r\n",
        "        def activation(x):\r\n",
        "            return x * (x > 0)\r\n",
        "\r\n",
        "        def activated_derivative(activated_x):\r\n",
        "            return np.float32(1) * (activated_x != 0)\r\n",
        "\r\n",
        "        return activation, activated_derivative\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def tanh(alpha=1):\r\n",
        "        def activation(x):\r\n",
        "            return np.arctan(alpha * x)\r\n",
        "\r\n",
        "        def activated_derivative(activated_x):\r\n",
        "            return alpha * np.square(np.cos(activated_x))\r\n",
        "\r\n",
        "        return activation, activated_derivative\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def softmax():\r\n",
        "        e = np.float32(np.e)\r\n",
        "\r\n",
        "        def activation(x):\r\n",
        "            numerator = e ** (x - x.max())\r\n",
        "\r\n",
        "            return numerator / np.einsum('ij->', numerator, dtype=np.float32)\r\n",
        "\r\n",
        "        def activated_derivative(activated_x):\r\n",
        "            j = -np.einsum('ij,kj', activated_x, activated_x)\r\n",
        "            j[np.diag_indices_from(j)] = np.einsum('ij,ij->ji', activated_x, (1 - activated_x))\r\n",
        "\r\n",
        "            return j.sum(axis=1, keepdims=1)\r\n",
        "\r\n",
        "        return activation, activated_derivative\r\n",
        "\r\n",
        "\r\n",
        "class Optimizer:\r\n",
        "    @staticmethod\r\n",
        "    def traditional_gradient_decent(this, lr):\r\n",
        "        def opt(l):\r\n",
        "            this.delta_weights[l], this.delta_biases[l] = lr * this.delta_weights[l], lr * this.delta_biases[l]\r\n",
        "\r\n",
        "        def optimizer():\r\n",
        "            this.find_delta(this.layers - 1, this.activated_output_derivative, this.weights[this.layers - 2])\r\n",
        "            [this.find_delta(l, this.activated_derivative, this.weights[l - 1]) for l in range(this.layers - 2, 0, -1)]\r\n",
        "\r\n",
        "        return optimizer, opt\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def moment(this, lr, alpha=None):\r\n",
        "        if alpha is None: alpha = lr\r\n",
        "        this.prev_delta_weights, this.prev_delta_biases = Initializer.normal(0)(this)\r\n",
        "\r\n",
        "        def opt(l):\r\n",
        "            this.delta_weights[l] = this.prev_delta_weights[l] = alpha * this.prev_delta_weights[l] + lr * \\\r\n",
        "                                                                 this.delta_weights[l]\r\n",
        "            this.delta_biases[l] = this.prev_delta_biases[l] = alpha * this.prev_delta_biases[l] + lr * \\\r\n",
        "                                                               this.delta_biases[l]\r\n",
        "\r\n",
        "        def optimizer():\r\n",
        "            this.find_delta(this.layers - 1, this.activated_output_derivative, this.weights[this.layers - 2])\r\n",
        "            [this.find_delta(l, this.activated_derivative, this.weights[l - 1]) for l in range(this.layers - 2, 0, -1)]\r\n",
        "\r\n",
        "        return optimizer, opt\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def decay(this, lr, alpha=None):\r\n",
        "        if alpha is None: alpha = lr\r\n",
        "\r\n",
        "        def opt(l):\r\n",
        "            k = lr / (1 + this.e / alpha)\r\n",
        "            this.delta_weights[l] = k * this.delta_weights[l]\r\n",
        "            this.delta_biases[l] = k * this.delta_biases[l]\r\n",
        "\r\n",
        "        def optimizer():\r\n",
        "            this.find_delta(this.layers - 1, this.activated_output_derivative, this.weights[this.layers - 2])\r\n",
        "            [this.find_delta(l, this.activated_derivative, this.weights[l - 1]) for l in range(this.layers - 2, 0, -1)]\r\n",
        "\r\n",
        "        return optimizer, opt\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def nesterov(this, lr, alpha=None):\r\n",
        "        if alpha is None: alpha = lr\r\n",
        "        this.prev_delta_weights, this.prev_delta_biases = Initializer.normal(0)(this)\r\n",
        "\r\n",
        "        def opt(l):\r\n",
        "            this.delta_weights[l] = this.prev_delta_weights[l] = alpha * this.prev_delta_weights[l] + lr * \\\r\n",
        "                                                                 this.delta_weights[l]\r\n",
        "            this.delta_biases[l] = this.prev_delta_biases[l] = alpha * this.prev_delta_biases[l] + lr * \\\r\n",
        "                                                               this.delta_biases[l]\r\n",
        "\r\n",
        "        def optimizer():\r\n",
        "            this.find_delta(this.layers - 1, this.activated_output_derivative,\r\n",
        "                            this.weights[this.layers - 2] - this.prev_delta_weights[this.layers - 2])\r\n",
        "            [this.find_delta(l, this.activated_derivative, this.weights[l - 1] - this.prev_delta_weights[l - 1])\r\n",
        "             for l in range(this.layers - 2, 0, -1)]\r\n",
        "\r\n",
        "        return optimizer, opt\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def adagrad(this, lr):\r\n",
        "        e = np.float32(np.e)\r\n",
        "        this.gti_w, this.gti_b = Initializer.normal(0)(this)\r\n",
        "\r\n",
        "        def opt(l):\r\n",
        "            d_w, d_b = this.delta_weights[l], this.delta_biases[l]\r\n",
        "            this.gti_w[l] += np.square(d_w)\r\n",
        "            this.gti_b[l] += np.square(d_b)\r\n",
        "\r\n",
        "            this.delta_weights[l] = lr * d_w / np.sqrt(e ** -8 + this.gti_w[l])\r\n",
        "            this.delta_biases[l] = lr * d_b / np.sqrt(e ** -8 + this.gti_b[l])\r\n",
        "\r\n",
        "        def optimizer():\r\n",
        "            this.find_delta(this.layers - 1, this.activated_output_derivative, this.weights[this.layers - 2])\r\n",
        "            [this.find_delta(l, this.activated_derivative, this.weights[l - 1]) for l in range(this.layers - 2, 0, -1)]\r\n",
        "\r\n",
        "        return optimizer, opt\r\n",
        "\r\n",
        "    # Know some what how this work, but not sure\r\n",
        "    @staticmethod\r\n",
        "    def adadelta(this, lr=0.01, alpha=0.95, epsilon=np.e ** -6):\r\n",
        "        this.vt_w, this.vt_b = Initializer.uniform(0, 1)(this)\r\n",
        "        this.wt_w, this.wt_b = Initializer.uniform(0, 1)(this)\r\n",
        "\r\n",
        "        def opt(l):\r\n",
        "            this.vt_w[l] = alpha * this.vt_w[l] + (1 - alpha) * np.square(this.delta_weights[l])\r\n",
        "            this.vt_b[l] = alpha * this.vt_b[l] + (1 - alpha) * np.square(this.delta_biases[l])\r\n",
        "            this.delta_weights[l] = np.sqrt(epsilon + this.wt_w[l]) * this.delta_weights[l] / np.sqrt(\r\n",
        "                epsilon + this.vt_w[l])\r\n",
        "            this.delta_biases[l] = np.sqrt(epsilon + this.wt_b[l]) * this.delta_biases[l] / np.sqrt(\r\n",
        "                epsilon + this.vt_b[l])\r\n",
        "            this.wt_w[l] = alpha * this.wt_w[l] + (1 - alpha) * np.square(this.delta_biases[l])\r\n",
        "            this.wt_b[l] = alpha * this.wt_b[l] + (1 - alpha) * np.square(this.delta_biases[l])\r\n",
        "\r\n",
        "            this.delta_weights[l] = lr * this.delta_weights[l]\r\n",
        "            this.delta_biases[l] = lr * this.delta_biases[l]\r\n",
        "\r\n",
        "        def optimizer():\r\n",
        "            this.find_delta(this.layers - 1, this.activated_output_derivative, this.weights[this.layers - 2])\r\n",
        "            [this.find_delta(l, this.activated_derivative, this.weights[l - 1]) for l in range(this.layers - 2, 0, -1)]\r\n",
        "\r\n",
        "        return optimizer, opt\r\n",
        "\r\n",
        "\r\n",
        "class LoadNeuralNetwork:\r\n",
        "    pass\r\n",
        "\r\n",
        "\r\n",
        "class SaveNeuralNetwork:\r\n",
        "    pass\r\n",
        "\r\n",
        "\r\n",
        "class PlotGraph:\r\n",
        "    @staticmethod\r\n",
        "    def plot_cost_graph(nn):\r\n",
        "        costs = []\r\n",
        "        i = 0\r\n",
        "        for cs in nn.costs:\r\n",
        "            costs.append([(c + i, j) for c, j in enumerate(cs)])\r\n",
        "            i += len(cs) - 1\r\n",
        "\r\n",
        "        lc = mc.LineCollection(costs, colors=['red', 'green'], linewidths=1)\r\n",
        "        sp = plt.subplot()\r\n",
        "        sp.add_collection(lc)\r\n",
        "\r\n",
        "        sp.autoscale()\r\n",
        "        sp.margins(0.1)\r\n",
        "        plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "XzEkHYJhF6ha"
      },
      "source": [
        "#@title unzipper\n",
        "# importing required modules \n",
        "from zipfile import ZipFile \n",
        "  \n",
        "# specifying the zip file name \n",
        "file_name = \"/content/drive/MyDrive/emnist.zip\"\n",
        "  \n",
        "# opening the zip file in READ mode \n",
        "with ZipFile(file_name, 'r') as zip: \n",
        "    # printing all the contents of the zip file \n",
        "    zip.printdir() \n",
        "  \n",
        "    # extracting all the files \n",
        "    print('Extracting all the files now...') \n",
        "    zip.extractall() \n",
        "    print('Done!') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "UVXZcOXanp9i"
      },
      "source": [
        "#@title load gz data\n",
        "import gzip\n",
        "import numpy as np\n",
        "\n",
        "tr_file = '/content/gzip/emnist-bymerge-train-images-idx3-ubyte.gz'\n",
        "tl_file = '/content/gzip/emnist-bymerge-train-labels-idx1-ubyte.gz'\n",
        "def training_images():\n",
        "    with gzip.open(tr_file, 'r') as f:\n",
        "        # first 4 bytes is a magic number\n",
        "        magic_number = int.from_bytes(f.read(4), 'big')\n",
        "        # second 4 bytes is the number of images\n",
        "        image_count = int.from_bytes(f.read(4), 'big')\n",
        "        # third 4 bytes is the row count\n",
        "        row_count = int.from_bytes(f.read(4), 'big')\n",
        "        # fourth 4 bytes is the column count\n",
        "        column_count = int.from_bytes(f.read(4), 'big')\n",
        "        # rest is the image pixel data, each pixel is stored as an unsigned byte\n",
        "        # pixel values are 0 to 255\n",
        "        image_data = f.read()\n",
        "        images = np.frombuffer(image_data, dtype=np.uint8)\\\n",
        "            .reshape((image_count, row_count, column_count))\n",
        "        return images\n",
        "\n",
        "\n",
        "def training_labels():\n",
        "    with gzip.open(tl_file, 'r') as f:\n",
        "        # first 4 bytes is a magic number\n",
        "        magic_number = int.from_bytes(f.read(4), 'big')\n",
        "        # second 4 bytes is the number of labels\n",
        "        label_count = int.from_bytes(f.read(4), 'big')\n",
        "        # rest is the label data, each label is stored as unsigned byte\n",
        "        # label values are 0 to 9\n",
        "        label_data = f.read()\n",
        "        labels = np.frombuffer(label_data, dtype=np.uint8)\n",
        "        return labels\n",
        "\n",
        "\n",
        "t, l = training_images(), training_labels()\n",
        "# ll = np.zeros((l.size, l.max()+1))\n",
        "# ll[np.arange(l.size),l] = 1\n",
        "# ts = zip(t, ll)\n",
        "# ttr = [[tr, la] for tr, la in ts]\n",
        "# [tr[0].resize(784, 1) for tr in ttr]\n",
        "# np.savez_compressed('/content/drive/MyDrive/Colab Notebooks/image_classification_47_balenced.npz', ttr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS_1c9KRyWDJ"
      },
      "source": [
        "np_reader = np.load('/content/drive/MyDrive/Colab Notebooks/image_classification_47_balenced.npz', allow_pickle=True)\r\n",
        "data = np_reader['arr_0']\r\n",
        "_ = [exec('d[0]=d[0]/256') for d in data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex5dWuKEAM0d"
      },
      "source": [
        "def show_image(img_data):\r\n",
        "    image = np.asarray(img_data).squeeze()\r\n",
        "    plt.imshow(image, cmap='Greys')\r\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DhjXKtl5pKw"
      },
      "source": [
        "nn = CreateNeuralNetwork(shape=(784, 392, 196, 47),\r\n",
        "                         initializer=Initializer.xavier(he=2),\r\n",
        "                         activation=ActivationFunction.relu(),\r\n",
        "                         output_activation=ActivationFunction.softmax())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3pvWzO4qJQV"
      },
      "source": [
        "nn_reader = np.load('/content/drive/MyDrive/Colab Notebooks/class_47_nn_20.npz', allow_pickle=True)\r\n",
        "nn.weights, nn.bases = nn_reader['arr_0'], nn_reader['arr_1']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjx1H-rz57an",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628f810c-d535-4eac-dc2c-895b74ba6ab1"
      },
      "source": [
        "nn.train(training_set=data,\r\n",
        "         epochs=4,\r\n",
        "         batch_size=-1,\r\n",
        "         loss_function=LossFunction.mean_square(),\r\n",
        "         optimizer=Optimizer.adagrad(nn, 0.01),\r\n",
        "         vectorize=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0  cost: 0.09306471705891094 time: 311.1301121711731\n",
            "epoch: 1  cost: 0.05383141202471649 time: 310.93207001686096\n",
            "epoch: 2  cost: 0.0479443050744816 time: 312.5070128440857\n",
            "epoch: 3  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4uTn9IV6fyA"
      },
      "source": [
        "nn.train(training_set=None,\r\n",
        "         epochs=5,\r\n",
        "         batch_size=None,\r\n",
        "         loss_function=None,\r\n",
        "         optimizer=None,\r\n",
        "         vectorize=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JClLVnIXf5vI"
      },
      "source": [
        "np.savez_compressed('/content/drive/MyDrive/Colab Notebooks/class_47_nn_4.1.npz', nn.weights, nn.biases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZHAbpKP-T9L"
      },
      "source": [
        "nn.training_set = None\r\n",
        "dl.settings['recurse']=True\r\n",
        "dl.dump(nn, open('nn_class_47_13.nn', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAbdwS2qAb_B"
      },
      "source": [
        "for i in range(20, 30):\r\n",
        "    show_image(t[i])\r\n",
        "    code = l[i]\r\n",
        "    if code < 10: code += 48\r\n",
        "    elif code < 10 + 26: code += 65 - 10\r\n",
        "    else: code += 98 - 10 - 26\r\n",
        "    print(chr(code))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
