{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_Networks_And_Its_Topologies.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMuedeeg/E43ctUFuTUiEzh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amith225/NEURAL_NETWORKS_AND_TOPOLOGIES/blob/master/scr/Neural_Networks_And_Its_Topologies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "714F-Rb0Ym1a",
        "cellView": "form"
      },
      "source": [
        "#@title Library Imports\r\n",
        "import time as tm\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import gzip\r\n",
        "\r\n",
        "from matplotlib import collections as mc, pyplot as plt\r\n",
        "\r\n",
        "from zipfile import ZipFile\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NVcHoj69Yak",
        "cellView": "form"
      },
      "source": [
        "#@title NeuralNetwork And Its Topologies Classes\r\n",
        "import time as tm\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "from matplotlib import collections as mc, pyplot as plt\r\n",
        "\r\n",
        "\r\n",
        "class CreateNeuralNetwork:\r\n",
        "    def __init__(self, shape, initializer, activation, output_activation=None):\r\n",
        "        self.shape = shape\r\n",
        "        self.layers = len(self.shape)\r\n",
        "        if output_activation is None: output_activation = activation\r\n",
        "        self.weights, self.biases = initializer(self)\r\n",
        "        self.activation, self.activated_derivative = activation\r\n",
        "        self.output_activation, self.activated_output_derivative = output_activation\r\n",
        "        self.activated_outputs = np.array([np.zeros((self.shape[i], 1), dtype=np.float32) for i in range(self.layers)],\r\n",
        "                                          dtype=np.ndarray)\r\n",
        "        self.delta_weights, self.delta_biases = Initializer.normal(0)(self)\r\n",
        "\r\n",
        "        self.costs = []\r\n",
        "        self.cost = 0\r\n",
        "        self.e = 0\r\n",
        "        self.train_database = None\r\n",
        "        self.epochs = None\r\n",
        "        self.batch_size = None\r\n",
        "        self.loss_function = None\r\n",
        "        self.optimizer = None\r\n",
        "        self.opt = None\r\n",
        "        self.cost_derivative = None\r\n",
        "\r\n",
        "    def process(self, input):\r\n",
        "        self.activated_outputs[0] = np.array(input, dtype=np.float32).reshape((len(input), 1))\r\n",
        "\r\n",
        "        for l in range(self.layers - 2):\r\n",
        "            self.activated_outputs[l + 1] = self.activation(\r\n",
        "                np.einsum('ij,jk->ik', self.weights[l], self.activated_outputs[l],\r\n",
        "                          dtype=np.float32) + self.biases[l])\r\n",
        "\r\n",
        "        return self.output_activation(np.einsum('ij,jk->ik', self.weights[l + 1], self.activated_outputs[l + 1],\r\n",
        "                                                dtype=np.float32) + self.biases[l])\r\n",
        "\r\n",
        "    def forward_pass(self, input):\r\n",
        "        self.activated_outputs[0] = input\r\n",
        "\r\n",
        "        for l in range(self.layers - 2):\r\n",
        "            self.activated_outputs[l + 1] = self.activation(\r\n",
        "                np.einsum('ij,jk->ik', self.weights[l], self.activated_outputs[l],\r\n",
        "                          dtype=np.float32) + self.biases[l])\r\n",
        "\r\n",
        "        self.activated_outputs[l + 2] = self.output_activation(\r\n",
        "            np.einsum('ij,jk->ik', self.weights[l + 1], self.activated_outputs[l + 1],\r\n",
        "                      dtype=np.float32) + self.biases[l + 1])\r\n",
        "\r\n",
        "    def delta_update(self, layer, activated_derivative, theta):\r\n",
        "        layer = layer - 1\r\n",
        "        self.delta_biases[layer] = self.cost_derivative * activated_derivative(self.activated_outputs[layer + 1])\r\n",
        "        np.einsum('ij,ji->ij', self.delta_biases[layer], self.activated_outputs[layer], dtype=np.float32,\r\n",
        "                  out=self.delta_weights[layer])\r\n",
        "\r\n",
        "        self.cost_derivative = np.einsum('ij,ik', theta, self.cost_derivative, dtype=np.float32)\r\n",
        "\r\n",
        "        self.opt(layer)\r\n",
        "        self.weights[layer] -= self.delta_weights[layer]\r\n",
        "        self.biases[layer] -= self.delta_biases[layer]\r\n",
        "\r\n",
        "    def train(self, train_database=None, epochs=None, batch_size=None, loss_function=None, optimizer=None):\r\n",
        "        if train_database is not None: self.train_database = train_database\r\n",
        "        if epochs is not None: self.epochs = epochs\r\n",
        "        if batch_size is not None: self.batch_size = batch_size\r\n",
        "        if loss_function is not None: self.loss_function = loss_function\r\n",
        "        if optimizer is not None: self.optimizer, self.opt = optimizer\r\n",
        "\r\n",
        "        train_costs = []\r\n",
        "        for e in range(self.epochs):\r\n",
        "            print('epoch:', e, end='  ')\r\n",
        "            batch_set = self.train_database.mini_batch(self.batch_size)\r\n",
        "            t = tm.time()\r\n",
        "            self.cost = 0\r\n",
        "            for b in zip(*batch_set):\r\n",
        "                self.optimizer(b)\r\n",
        "            cost = self.cost / self.train_database.batch_size\r\n",
        "            print('cost:', cost, 'time:', tm.time() - t)\r\n",
        "            train_costs.append(cost)\r\n",
        "\r\n",
        "            self.e += e * self.train_database.batch_size / self.train_database.shape[0]\r\n",
        "        self.costs.append(train_costs)\r\n",
        "\r\n",
        "        self.activated_outputs = np.array([np.zeros((self.shape[i], 1), dtype=np.float32) for i in range(self.layers)],\r\n",
        "                                          dtype=np.ndarray)\r\n",
        "\r\n",
        "    def test(self):\r\n",
        "        pass\r\n",
        "\r\n",
        "\r\n",
        "class Initializer:\r\n",
        "    @staticmethod\r\n",
        "    def uniform(start, stop):\r\n",
        "        def initializer(self):\r\n",
        "            weights = [np.random.uniform(start, stop, (self.shape[i], self.shape[i - 1])).astype(dtype=np.float32)\r\n",
        "                       for i in range(1, self.layers)]\r\n",
        "            biases = [np.random.uniform(start, stop, (self.shape[i], 1)).astype(dtype=np.float32)\r\n",
        "                      for i in range(1, self.layers)]\r\n",
        "\r\n",
        "            return weights, biases\r\n",
        "\r\n",
        "        return initializer\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def normal(scale=1):\r\n",
        "        def initializer(self):\r\n",
        "            weights = [(np.random.default_rng().standard_normal((self.shape[i], self.shape[i - 1]),\r\n",
        "                                                                dtype=np.float32)) * scale\r\n",
        "                       for i in range(1, self.layers)]\r\n",
        "            biases = [(np.random.default_rng().standard_normal((self.shape[i], 1),\r\n",
        "                                                               dtype=np.float32)) * scale\r\n",
        "                      for i in range(1, self.layers)]\r\n",
        "\r\n",
        "            return weights, biases\r\n",
        "\r\n",
        "        return initializer\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def xavier(he=1):\r\n",
        "        def initializer(self):\r\n",
        "            weights = [np.random.default_rng().standard_normal((self.shape[i], self.shape[i - 1]),\r\n",
        "                                                               dtype=np.float32) * (he / self.shape[i - 1]) ** 0.5\r\n",
        "                       for i in range(1, self.layers)]\r\n",
        "            biases = [np.random.default_rng().standard_normal((self.shape[i], 1),\r\n",
        "                                                              dtype=np.float32) * (he / self.shape[i - 1]) ** 0.5\r\n",
        "                      for i in range(1, self.layers)]\r\n",
        "\r\n",
        "            return weights, biases\r\n",
        "\r\n",
        "        return initializer\r\n",
        "\r\n",
        "\r\n",
        "class LossFunction:\r\n",
        "    @staticmethod\r\n",
        "    def mean_square():\r\n",
        "        def loss_function(self, b):\r\n",
        "            self.forward_pass(b[0])\r\n",
        "            cost_derivative = self.activated_outputs[-1] - b[1]\r\n",
        "\r\n",
        "            return cost_derivative, np.einsum('ij,ij->', cost_derivative, cost_derivative, dtype=np.float32)\r\n",
        "\r\n",
        "        return loss_function\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def cross_entropy():\r\n",
        "        def loss_function(self, b):\r\n",
        "            pass\r\n",
        "\r\n",
        "        return loss_function\r\n",
        "\r\n",
        "\r\n",
        "class ActivationFunction:\r\n",
        "    @staticmethod\r\n",
        "    def sigmoid(alpha=1, beta=0):\r\n",
        "        e = np.float32(np.e)\r\n",
        "\r\n",
        "        def activation(x):\r\n",
        "            return 1 / (1 + e ** (-alpha * (x + beta)))\r\n",
        "\r\n",
        "        def activated_derivative(activated_x):\r\n",
        "            return alpha * (activated_x * (1 - activated_x))\r\n",
        "\r\n",
        "        return activation, activated_derivative\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def relu():\r\n",
        "        def activation(x):\r\n",
        "            return x * (x > 0)\r\n",
        "\r\n",
        "        def activated_derivative(activated_x):\r\n",
        "            return np.float32(1) * (activated_x != 0)\r\n",
        "\r\n",
        "        return activation, activated_derivative\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def tanh(alpha=1):\r\n",
        "        def activation(x):\r\n",
        "            return np.arctan(alpha * x)\r\n",
        "\r\n",
        "        def activated_derivative(activated_x):\r\n",
        "            return alpha * np.square(np.cos(activated_x))\r\n",
        "\r\n",
        "        return activation, activated_derivative\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def softmax():\r\n",
        "        e = np.float32(np.e)\r\n",
        "\r\n",
        "        def activation(x):\r\n",
        "            numerator = e ** (x - x.max())\r\n",
        "\r\n",
        "            return numerator / np.einsum('ij->', numerator, dtype=np.float32)\r\n",
        "\r\n",
        "        def activated_derivative(activated_x):\r\n",
        "            j = -np.einsum('ij,kj', activated_x, activated_x)\r\n",
        "            j[np.diag_indices_from(j)] = np.einsum('ij,ij->ji', activated_x, (1 - activated_x))\r\n",
        "\r\n",
        "            return j.sum(axis=1, keepdims=1)\r\n",
        "\r\n",
        "        return activation, activated_derivative\r\n",
        "\r\n",
        "\r\n",
        "class Optimizer:\r\n",
        "    @staticmethod\r\n",
        "    def traditional_gradient_decent(this, lr):\r\n",
        "        def opt(l):\r\n",
        "            this.delta_weights[l], this.delta_biases[l] = lr * this.delta_weights[l], lr * this.delta_biases[l]\r\n",
        "\r\n",
        "        def optimizer(b):\r\n",
        "            this.cost_derivative, cost = this.loss_function(this, b)\r\n",
        "            this.cost += cost\r\n",
        "\r\n",
        "            this.delta_update(this.layers - 1, this.activated_output_derivative, this.weights[this.layers - 2])\r\n",
        "            [this.delta_update(l, this.activated_derivative, this.weights[l - 1]) for l in\r\n",
        "             range(this.layers - 2, 0, -1)]\r\n",
        "\r\n",
        "        return optimizer, opt\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def moment(this, lr, alpha=None):\r\n",
        "        if alpha is None: alpha = lr\r\n",
        "        this.prev_delta_weights, this.prev_delta_biases = Initializer.normal(0)(this)\r\n",
        "\r\n",
        "        def opt(l):\r\n",
        "            this.delta_weights[l] = this.prev_delta_weights[l] = alpha * this.prev_delta_weights[l] + lr * \\\r\n",
        "                                                                 this.delta_weights[l]\r\n",
        "            this.delta_biases[l] = this.prev_delta_biases[l] = alpha * this.prev_delta_biases[l] + lr * \\\r\n",
        "                                                               this.delta_biases[l]\r\n",
        "\r\n",
        "        def optimizer(b):\r\n",
        "            this.cost_derivative, cost = this.loss_function(this, b)\r\n",
        "            this.cost += cost\r\n",
        "\r\n",
        "            this.delta_update(this.layers - 1, this.activated_output_derivative, this.weights[this.layers - 2])\r\n",
        "            [this.delta_update(l, this.activated_derivative, this.weights[l - 1]) for l in\r\n",
        "             range(this.layers - 2, 0, -1)]\r\n",
        "\r\n",
        "        return optimizer, opt\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def decay(this, lr, alpha=None):\r\n",
        "        if alpha is None: alpha = lr\r\n",
        "\r\n",
        "        def opt(l):\r\n",
        "            k = lr / (1 + this.e / alpha)\r\n",
        "            this.delta_weights[l] = k * this.delta_weights[l]\r\n",
        "            this.delta_biases[l] = k * this.delta_biases[l]\r\n",
        "\r\n",
        "        def optimizer(b):\r\n",
        "            this.cost_derivative, cost = this.loss_function(this, b)\r\n",
        "            this.cost += cost\r\n",
        "\r\n",
        "            this.delta_update(this.layers - 1, this.activated_output_derivative, this.weights[this.layers - 2])\r\n",
        "            [this.delta_update(l, this.activated_derivative, this.weights[l - 1]) for l in\r\n",
        "             range(this.layers - 2, 0, -1)]\r\n",
        "\r\n",
        "        return optimizer, opt\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def nesterov(this, lr, alpha=None):\r\n",
        "        if alpha is None: alpha = lr\r\n",
        "        this.prev_delta_weights, this.prev_delta_biases = Initializer.normal(0)(this)\r\n",
        "\r\n",
        "        def opt(l):\r\n",
        "            this.delta_weights[l] = this.prev_delta_weights[l] = alpha * this.prev_delta_weights[l] + lr * \\\r\n",
        "                                                                 this.delta_weights[l]\r\n",
        "            this.delta_biases[l] = this.prev_delta_biases[l] = alpha * this.prev_delta_biases[l] + lr * \\\r\n",
        "                                                               this.delta_biases[l]\r\n",
        "\r\n",
        "        def optimizer(b):\r\n",
        "            this.cost_derivative, cost = this.loss_function(this, b)\r\n",
        "            this.cost += cost\r\n",
        "\r\n",
        "            this.delta_update(this.layers - 1, this.activated_output_derivative,\r\n",
        "                              this.weights[this.layers - 2] - this.prev_delta_weights[this.layers - 2])\r\n",
        "            [this.delta_update(l, this.activated_derivative, this.weights[l - 1] - this.prev_delta_weights[l - 1])\r\n",
        "             for l in range(this.layers - 2, 0, -1)]\r\n",
        "\r\n",
        "        return optimizer, opt\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def adagrad(this, lr=0.01, epsilon=np.e ** -8):\r\n",
        "        this.gti_w, this.gti_b = Initializer.normal(0)(this)\r\n",
        "\r\n",
        "        def opt(l):\r\n",
        "            d_w, d_b = this.delta_weights[l], this.delta_biases[l]\r\n",
        "            this.gti_w[l] += np.square(d_w)\r\n",
        "            this.gti_b[l] += np.square(d_b)\r\n",
        "\r\n",
        "            this.delta_weights[l] = lr * d_w / np.sqrt(epsilon + this.gti_w[l])\r\n",
        "            this.delta_biases[l] = lr * d_b / np.sqrt(epsilon + this.gti_b[l])\r\n",
        "\r\n",
        "        def optimizer(b):\r\n",
        "            this.cost_derivative, cost = this.loss_function(this, b)\r\n",
        "            this.cost += cost\r\n",
        "\r\n",
        "            this.delta_update(this.layers - 1, this.activated_output_derivative, this.weights[this.layers - 2])\r\n",
        "            [this.delta_update(l, this.activated_derivative, this.weights[l - 1]) for l in\r\n",
        "             range(this.layers - 2, 0, -1)]\r\n",
        "\r\n",
        "        return optimizer, opt\r\n",
        "\r\n",
        "    # Know some what how this work, but not sure\r\n",
        "    @staticmethod\r\n",
        "    def adadelta(this, lr=1, alpha=0.95, epsilon=np.e ** -16):\r\n",
        "        alpha_bar = 1 - alpha\r\n",
        "        this.vt_w, this.vt_b = Initializer.normal(0)(this)\r\n",
        "        this.wt_w, this.wt_b = Initializer.normal(0)(this)\r\n",
        "\r\n",
        "        def opt(l):\r\n",
        "            this.vt_w[l] = alpha * this.vt_w[l] + alpha_bar * np.square(this.delta_weights[l])\r\n",
        "            this.vt_b[l] = alpha * this.vt_b[l] + alpha_bar * np.square(this.delta_biases[l])\r\n",
        "            this.delta_weights[l] = np.sqrt((epsilon + this.wt_w[l]) / (epsilon + this.vt_w[l])) * this.delta_weights[l]\r\n",
        "            this.delta_biases[l] = np.sqrt((epsilon + this.wt_b[l]) / (epsilon + this.vt_b[l])) * this.delta_biases[l]\r\n",
        "            this.wt_w[l] = alpha * this.wt_w[l] + alpha_bar * np.square(this.delta_biases[l])\r\n",
        "            this.wt_b[l] = alpha * this.wt_b[l] + alpha_bar * np.square(this.delta_biases[l])\r\n",
        "\r\n",
        "            this.delta_weights[l] = lr * this.delta_weights[l]\r\n",
        "            this.delta_biases[l] = lr * this.delta_biases[l]\r\n",
        "\r\n",
        "        def optimizer(b):\r\n",
        "            this.cost_derivative, cost = this.loss_function(this, b)\r\n",
        "            this.cost += cost\r\n",
        "\r\n",
        "            this.delta_update(this.layers - 1, this.activated_output_derivative, this.weights[this.layers - 2])\r\n",
        "            [this.delta_update(l, this.activated_derivative, this.weights[l - 1]) for l in\r\n",
        "             range(this.layers - 2, 0, -1)]\r\n",
        "\r\n",
        "        return optimizer, opt\r\n",
        "\r\n",
        "    # no understanding at all\r\n",
        "    @staticmethod\r\n",
        "    def adam(this, lr=0.005, alpha=0.9, beta=0.999, epsilon=np.e ** -16):\r\n",
        "        alpha_bar = 1 - alpha\r\n",
        "        beta_bar = 1 - beta\r\n",
        "        this.mt_w, this.mt_b = Initializer.normal(0)(this)\r\n",
        "        this.vt_w, this.vt_b = Initializer.normal(0)(this)\r\n",
        "        this.t = 0\r\n",
        "\r\n",
        "        def opt(l):\r\n",
        "            this.t += 1\r\n",
        "            this.mt_w[l] = alpha * this.mt_w[l] + alpha_bar * this.delta_weights[l]\r\n",
        "            this.mt_b[l] = alpha * this.mt_b[l] + alpha_bar * this.delta_biases[l]\r\n",
        "            this.vt_w[l] = beta * this.vt_w[l] + beta_bar * (this.delta_weights[l] ** 2)\r\n",
        "            this.vt_b[l] = beta * this.vt_b[l] + beta_bar * (this.delta_biases[l] ** 2)\r\n",
        "\r\n",
        "            m_dw_corr = this.mt_w[l] / (1 - alpha ** this.t)\r\n",
        "            m_db_corr = this.mt_b[l] / (1 - alpha ** this.t)\r\n",
        "            v_dw_corr = this.vt_w[l] / (1 - beta ** this.t)\r\n",
        "            v_db_corr = this.vt_b[l] / (1 - beta ** this.t)\r\n",
        "\r\n",
        "            this.delta_weights[l] = lr * (m_dw_corr / (np.sqrt(v_dw_corr) + epsilon))\r\n",
        "            this.delta_biases[l] = lr * (m_db_corr / (np.sqrt(v_db_corr) + epsilon))\r\n",
        "\r\n",
        "        def optimizer(b):\r\n",
        "            this.cost_derivative, cost = this.loss_function(this, b)\r\n",
        "            this.cost += cost\r\n",
        "\r\n",
        "            this.delta_update(this.layers - 1, this.activated_output_derivative, this.weights[this.layers - 2])\r\n",
        "            [this.delta_update(l, this.activated_derivative, this.weights[l - 1]) for l in\r\n",
        "             range(this.layers - 2, 0, -1)]\r\n",
        "\r\n",
        "        return optimizer, opt\r\n",
        "\r\n",
        "\r\n",
        "class CreateDatabase:\r\n",
        "    def __init__(self, input_data, labels):\r\n",
        "        self.input_data = np.array(list(input_data), dtype=np.float32).reshape((len(input_data), len(input_data[0]), 1))\r\n",
        "        self.labels = np.array(list(labels), dtype=np.float32).reshape((len(labels), len(labels[0]), 1))\r\n",
        "\r\n",
        "        self.shape = self.input_data.shape\r\n",
        "\r\n",
        "        self.batch_size = -1\r\n",
        "        self.mini_batch_i = 0\r\n",
        "\r\n",
        "    def mini_batch(self, batch_size):\r\n",
        "        if batch_size < 0:\r\n",
        "            self.batch_size = self.shape[0]\r\n",
        "        else:\r\n",
        "            self.batch_size = batch_size\r\n",
        "\r\n",
        "        batch_i = np.random.choice(self.shape[0], self.batch_size, replace=False)\r\n",
        "\r\n",
        "        return self.input_data[batch_i], self.labels[batch_i]\r\n",
        "\r\n",
        "\r\n",
        "class LoadNeuralNetwork:\r\n",
        "    pass\r\n",
        "\r\n",
        "\r\n",
        "class SaveNeuralNetwork:\r\n",
        "    pass\r\n",
        "\r\n",
        "\r\n",
        "class PlotGraph:\r\n",
        "    @staticmethod\r\n",
        "    def plot_cost_graph(nn):\r\n",
        "        costs = []\r\n",
        "        i = 0\r\n",
        "        for cs in nn.costs:\r\n",
        "            costs.append([(c + i, j) for c, j in enumerate(cs)])\r\n",
        "            i += len(cs) - 1\r\n",
        "\r\n",
        "        lc = mc.LineCollection(costs, colors=['red', 'green'], linewidths=1)\r\n",
        "        sp = plt.subplot()\r\n",
        "        sp.add_collection(lc)\r\n",
        "\r\n",
        "        sp.autoscale()\r\n",
        "        sp.margins(0.1)\r\n",
        "        plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vogfWdZt9bt6",
        "cellView": "form"
      },
      "source": [
        "#@title Unzipper\r\n",
        "# zip file path\r\n",
        "file_name = \"/content/drive/MyDrive/emnist.zip\"\r\n",
        "  \r\n",
        "# opening the zip file in READ mode \r\n",
        "with ZipFile(file_name, 'r') as z: \r\n",
        "    # printing all the contents of the zip file \r\n",
        "    z.printdir() \r\n",
        "  \r\n",
        "    # extracting all the files \r\n",
        "    print('Extracting all the files now...') \r\n",
        "    z.extractall() \r\n",
        "    print('Done!') \r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BwT0upW9hiH",
        "cellView": "form"
      },
      "source": [
        "#@title Load gz data and formating\r\n",
        "# your gz files or files extracted from emnist.zip\r\n",
        "tr_file = '/content/gzip/emnist-balanced-train-images-idx3-ubyte.gz'\r\n",
        "tl_file = '/content/gzip/emnist-balanced-train-labels-idx1-ubyte.gz'\r\n",
        "def training_images():\r\n",
        "    with gzip.open(tr_file, 'r') as f:\r\n",
        "        # first 4 bytes is a magic number\r\n",
        "        magic_number = int.from_bytes(f.read(4), 'big')\r\n",
        "        # second 4 bytes is the number of images\r\n",
        "        image_count = int.from_bytes(f.read(4), 'big')\r\n",
        "        # third 4 bytes is the row count\r\n",
        "        row_count = int.from_bytes(f.read(4), 'big')\r\n",
        "        # fourth 4 bytes is the column count\r\n",
        "        column_count = int.from_bytes(f.read(4), 'big')\r\n",
        "        # rest is the image pixel data, each pixel is stored as an unsigned byte\r\n",
        "        # pixel values are 0 to 255\r\n",
        "        image_data = f.read()\r\n",
        "        images = np.frombuffer(image_data, dtype=np.uint8)\\\r\n",
        "            .reshape((image_count, row_count, column_count))\r\n",
        "        return images\r\n",
        "\r\n",
        "\r\n",
        "def training_labels():\r\n",
        "    with gzip.open(tl_file, 'r') as f:\r\n",
        "        # first 4 bytes is a magic number\r\n",
        "        magic_number = int.from_bytes(f.read(4), 'big')\r\n",
        "        # second 4 bytes is the number of labels\r\n",
        "        label_count = int.from_bytes(f.read(4), 'big')\r\n",
        "        # rest is the label data, each label is stored as unsigned byte\r\n",
        "        # label values are 0 to 9\r\n",
        "        label_data = f.read()\r\n",
        "        labels = np.frombuffer(label_data, dtype=np.uint8)\r\n",
        "        return labels\r\n",
        "\r\n",
        "\r\n",
        "training_img, labels_img = training_images() / 256, training_labels()\r\n",
        "labels = np.zeros((labels_img.size, labels_img.max()+1))\r\n",
        "labels[np.arange(labels_img.size), labels_img] = 1\r\n",
        "training_img.resize((len(training_img), 784, 1))\r\n",
        "\r\n",
        "# uncomment to save at your path\r\n",
        "np.savez_compressed('/content/drive/MyDrive/Colab Notebooks/image_classification_47_balenced.npz', training_img, labels)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZTJWBv8_VMU",
        "cellView": "form"
      },
      "source": [
        "#@title Load formatted data file\r\n",
        "# your formatted data file path\r\n",
        "np_reader = np.load('/content/drive/MyDrive/Colab Notebooks/image_classification_47_balenced.npz', allow_pickle=True)\r\n",
        "data = np_reader['arr_0'], np_reader['arr_1']\r\n",
        "data = CreateDatabase(*data)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki6eUrym_zA2",
        "cellView": "form"
      },
      "source": [
        "#@title Show the image from formatted data\r\n",
        "def show_image(img_data):\r\n",
        "    img_data = img_data.reshape((28, 28))\r\n",
        "    image = np.asarray(img_data).squeeze()\r\n",
        "    plt.imshow(image, cmap='Greys')\r\n",
        "    plt.show()\r\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJIsw6SNAGKB"
      },
      "source": [
        "nn = CreateNeuralNetwork(shape=(784, 392, 196, 47),\r\n",
        "                         initializer=Initializer.xavier(he=2),\r\n",
        "                         activation=ActivationFunction.relu(),\r\n",
        "                         output_activation=ActivationFunction.softmax())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "bUW4fOGFAWSv"
      },
      "source": [
        "#@title load saved neural network(nn)\r\n",
        "# your saved nn file path\r\n",
        "nn_reader = np.load('/content/drive/MyDrive/Colab Notebooks/class_47_nn_20.npz', allow_pickle=True)\r\n",
        "nn.weights, nn.bases = nn_reader['arr_0'], nn_reader['arr_1']\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyLVnB78AnEp",
        "outputId": "bc2955a1-efaf-4dc1-cd0a-09ac87190176",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# initialize training hyperparameters\r\n",
        "nn.train(train_database=data,\r\n",
        "         epochs=4,\r\n",
        "         batch_size=-1,\r\n",
        "         loss_function=LossFunction.mean_square(),\r\n",
        "         optimizer=Optimizer.adagrad(nn, 0.01))\r\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0  cost: 0.173148046167499 time: 288.2158224582672\n",
            "epoch: 1  cost: 0.13697012671221798 time: 292.76219296455383\n",
            "epoch: 2  cost: 0.1279429105190867 time: 294.25938272476196\n",
            "epoch: 3  cost: 0.12233021499056784 time: 319.541273355484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPaSfukNAq9v",
        "outputId": "d20ac7c8-ba0b-461f-cb0f-7872ed735d03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# quick train(uses previously used hyperparameters)\r\n",
        "# change any hyperparameter wanted\r\n",
        "nn.train(train_database=data,\r\n",
        "         epochs=None,\r\n",
        "         batch_size=None,\r\n",
        "         loss_function=None,\r\n",
        "         optimizer=None)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0  "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0k-3qd3GaV1"
      },
      "source": [
        "# save a nn\r\n",
        "np.savez_compressed('/content/drive/MyDrive/Colab Notebooks/class_47_nn_autosave4.npz', nn.weights, nn.biases)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-DLMN_6BKBH"
      },
      "source": [
        "# your code here\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}