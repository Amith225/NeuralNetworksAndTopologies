{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1H4lo20YL0Ej_s-6dpholOXW344MFqByg",
      "authorship_tag": "ABX9TyPQYzbbbfJDn1TVE4DrXl39",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amith225/NEURAL_NETWORKS_AND_TOPOLOGIES/blob/master/scr/nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqHCw7xNWl03"
      },
      "source": [
        "import time as tm\n",
        " \n",
        "import numpy as np\n",
        " \n",
        "from matplotlib import collections as mc, pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9SuHfumJSLt"
      },
      "source": [
        "class CreateNeuralNetwork:\n",
        "    def __init__(self, shape, initializer, activation, output_activation=None):\n",
        "        self.shape = shape\n",
        "        self.layers = len(self.shape)\n",
        "        if output_activation is None: output_activation = activation\n",
        "        self.weights, self.biases = initializer(self)\n",
        "        self.activation, self.activated_derivative = activation\n",
        "        self.output_activation, self.activated_output_derivative = output_activation\n",
        "        self.activated_outputs = np.array([np.zeros((self.shape[i], 1), dtype=np.float32) for i in range(self.layers)],\n",
        "                                          dtype=np.ndarray)\n",
        "        self.delta_weights, self.delta_biases = Initializer.normal(0)(self)\n",
        " \n",
        "        self.costs = []\n",
        "        self.cost = 0\n",
        "        self.e = 0\n",
        "        self.training_set = None\n",
        "        self.epochs = None\n",
        "        self.batch_size = None\n",
        "        self.loss_function = None\n",
        "        self.optimizer = None\n",
        "        self.cost_derivative = None\n",
        " \n",
        "    def process(self, input):\n",
        "        input = np.array(input, dtype=np.float32).reshape((len(input), 1))\n",
        " \n",
        "        for l in range(self.layers - 2):\n",
        "            input = self.activated_outputs[l + 1] = \\\n",
        "                self.activation(np.einsum('ij,jk->ik', self.weights[l], input, dtype=np.float32) + self.biases[l])\n",
        " \n",
        "        return self.output_activation(\n",
        "            np.einsum('ij,jk->ik', self.weights[l + 1], input, dtype=np.float32) + self.biases[l + 1])\n",
        " \n",
        "    def forward_pass(self, input):\n",
        "        self.activated_outputs[0] = input\n",
        " \n",
        "        for l in range(self.layers - 2):\n",
        "            input = self.activated_outputs[l + 1] = \\\n",
        "                self.activation(np.einsum('ij,jk->ik', self.weights[l], input, dtype=np.float32) + self.biases[l])\n",
        " \n",
        "        self.activated_outputs[l + 2] = \\\n",
        "            self.output_activation(\n",
        "                np.einsum('ij,jk->ik', self.weights[l + 1], input, dtype=np.float32) + self.biases[l + 1])\n",
        " \n",
        "    def back_propagation(self, b):\n",
        "        self.cost_derivative, cost = self.loss_function(self, b)\n",
        "        self.cost += cost\n",
        " \n",
        "        self.find_delta(self.layers - 1, self.activated_output_derivative)\n",
        "        [self.find_delta(l, self.activated_derivative) for l in range(self.layers - 2, 0, -1)]\n",
        " \n",
        "        self.optimizer()\n",
        " \n",
        "    def find_delta(self, layer, activated_derivative):\n",
        "        delta_biases = self.cost_derivative * activated_derivative(self.activated_outputs[layer])\n",
        "        self.delta_biases[layer - 1] = delta_biases\n",
        "        np.einsum('ij,ji->ij', delta_biases, self.activated_outputs[layer - 1], dtype=np.float32,\n",
        "                  out=self.delta_weights[layer - 1])\n",
        " \n",
        "        self.cost_derivative = np.einsum('ij,ik', self.weights[layer - 1], self.cost_derivative, dtype=np.float32)\n",
        " \n",
        "    def train(self, training_set=None, epochs=None, batch_size=None, loss_function=None, optimizer=None,\n",
        "              vectorize=True):\n",
        "        if vectorize is True and training_set is not None:\n",
        "            training_set = np.array([[np.array(t[0], dtype=np.float32).reshape((len(t[0]), 1)),\n",
        "                                      np.array(t[1], dtype=np.float32).reshape((len(t[1]), 1))]\n",
        "                                     for t in training_set], dtype=np.ndarray)\n",
        "        if training_set is not None: self.training_set = training_set\n",
        "        if epochs is not None: self.epochs = epochs\n",
        "        if batch_size is not None: self.batch_size = batch_size\n",
        "        if loss_function is not None: self.loss_function = loss_function\n",
        "        if optimizer is not None: self.optimizer = optimizer\n",
        " \n",
        "        if self.batch_size < 0:\n",
        "            batch_size = len(self.training_set) + self.batch_size\n",
        "        else:\n",
        "            batch_size = self.batch_size\n",
        " \n",
        "        train_costs = []\n",
        "        for e in range(self.epochs):\n",
        "            print('epoch:', e, end='  ')\n",
        "            batch_set = self.training_set[np.random.choice(self.training_set.shape[0], batch_size, replace=False)]\n",
        " \n",
        "            t = tm.time()\n",
        "            self.cost = 0\n",
        "            for b in batch_set:\n",
        "                self.back_propagation(b)\n",
        "            cost = self.cost / batch_size\n",
        "            print('cost:', cost, 'time:', tm.time() - t)\n",
        "            train_costs.append(cost)\n",
        " \n",
        "            self.e += e * batch_size / len(self.training_set)\n",
        "        self.costs.append(train_costs)\n",
        " \n",
        "        self.activated_outputs = np.array([np.zeros((self.shape[i], 1), dtype=np.float32) for i in range(self.layers)],\n",
        "                                          dtype=np.ndarray)\n",
        " \n",
        "    def test(self):\n",
        "        pass\n",
        " \n",
        " \n",
        "class Initializer:\n",
        "    @staticmethod\n",
        "    def normal(scale=1):\n",
        "        def initializer(self):\n",
        "            weights = [np.random.default_rng().standard_normal((self.shape[i], self.shape[i - 1]),\n",
        "                                                               dtype=np.float32) * scale\n",
        "                       for i in range(1, self.layers)]\n",
        "            biases = [np.random.default_rng().standard_normal((self.shape[i], 1),\n",
        "                                                              dtype=np.float32)\n",
        "                      for i in range(1, self.layers)]\n",
        " \n",
        "            return np.array(weights, dtype=np.ndarray), np.array(biases, dtype=np.ndarray)\n",
        " \n",
        "        return initializer\n",
        " \n",
        "    @staticmethod\n",
        "    def xavier(he=1):\n",
        "        def initializer(self):\n",
        "            weights = [np.random.default_rng().standard_normal((self.shape[i], self.shape[i - 1]),\n",
        "                                                               dtype=np.float32) * (he / self.shape[i - 1]) ** 0.5\n",
        "                       for i in range(1, self.layers)]\n",
        "            biases = [np.random.default_rng().standard_normal((self.shape[i], 1),\n",
        "                                                              dtype=np.float32) * (he / self.shape[i - 1]) ** 0.5\n",
        "                      for i in range(1, self.layers)]\n",
        " \n",
        "            return np.array(weights, dtype=np.ndarray), np.array(biases, dtype=np.ndarray)\n",
        " \n",
        "        return initializer\n",
        " \n",
        " \n",
        "class LossFunction:\n",
        "    @staticmethod\n",
        "    def mean_square():\n",
        "        def loss_function(self, b):\n",
        "            self.forward_pass(b[0])\n",
        "            cost_derivative = self.activated_outputs[-1] - b[1]\n",
        " \n",
        "            return cost_derivative, np.einsum('ij,ij->', cost_derivative, cost_derivative, dtype=np.float32)\n",
        " \n",
        "        return loss_function\n",
        " \n",
        " \n",
        "class ActivationFunction:\n",
        "    @staticmethod\n",
        "    def sigmoid(alpha=1, beta=0):\n",
        "        def activation(x):\n",
        "            return 1 / (1 + np.e ** (-alpha * (x + beta)))\n",
        " \n",
        "        def activated_derivative(activated_x):\n",
        "            return alpha * (activated_x * (1 - activated_x))\n",
        " \n",
        "        return activation, activated_derivative\n",
        " \n",
        "    @staticmethod\n",
        "    def relu():\n",
        "        def activation(x):\n",
        "            return x * (x > 0)\n",
        " \n",
        "        def activated_derivative(activated_x):\n",
        "            return np.float32(1) * (activated_x != 0)\n",
        " \n",
        "        return activation, activated_derivative\n",
        " \n",
        "    @staticmethod\n",
        "    def tanh(alpha=1):\n",
        "        def activation(x):\n",
        "            return np.arctan(alpha * x)\n",
        " \n",
        "        def activated_derivative(activated_x):\n",
        "            return alpha * np.cos(activated_x) ** 2\n",
        " \n",
        "        return activation, activated_derivative\n",
        " \n",
        "    @staticmethod\n",
        "    def softmax():\n",
        "        def activation(x):\n",
        "            numerator = np.e ** (x - x.max())\n",
        " \n",
        "            return numerator / np.einsum('ij->', numerator, dtype=np.float32)\n",
        " \n",
        "        def activated_derivative(activated_x):\n",
        "            return activated_x * (1 - activated_x)\n",
        " \n",
        "        return activation, activated_derivative\n",
        " \n",
        " \n",
        "class Optimizer:\n",
        "    @staticmethod\n",
        "    def learning_rate(this, lr):\n",
        "        def optimizer():\n",
        "            this.weights -= lr * this.delta_weights\n",
        "            this.biases -= lr * this.delta_biases\n",
        " \n",
        "        return optimizer\n",
        " \n",
        "    @staticmethod\n",
        "    def moment(this, lr, alpha=None):\n",
        "        if alpha is None: alpha = lr\n",
        "        this.prev_delta_weights, this.prev_delta_biases = Initializer.normal(0)(this)\n",
        " \n",
        "        def optimizer():\n",
        "            this.delta_weights = this.prev_delta_weights = alpha * this.prev_delta_weights + lr * this.delta_weights\n",
        "            this.delta_biases = this.prev_delta_biases = alpha * this.prev_delta_biases + lr * this.delta_biases\n",
        " \n",
        "            this.weights -= this.delta_weights\n",
        "            this.biases -= this.delta_biases\n",
        " \n",
        "        return optimizer\n",
        " \n",
        "    @staticmethod\n",
        "    def decay(this, lr, alpha=None):\n",
        "        if alpha is None: alpha = lr\n",
        " \n",
        "        def optimizer():\n",
        "            k = lr / (1 + this.e / alpha)\n",
        "            this.weights -= k * this.delta_weights\n",
        "            this.biases -= k * this.delta_biases\n",
        " \n",
        "        return optimizer\n",
        " \n",
        " \n",
        "class LoadNeuralNetwork:\n",
        "    pass\n",
        " \n",
        " \n",
        "class SaveNeuralNetwork:\n",
        "    pass\n",
        " \n",
        " \n",
        "class PlotGraph:\n",
        "    @staticmethod\n",
        "    def plot_cost_graph(nn):\n",
        "        costs = []\n",
        "        i = 0\n",
        "        for cs in nn.costs:\n",
        "            costs.append([(c + i, j) for c, j in enumerate(cs)])\n",
        "            i += len(cs) - 1\n",
        " \n",
        "        lc = mc.LineCollection(costs, colors=['red', 'green'], linewidths=1)\n",
        "        sp = plt.subplot()\n",
        "        sp.add_collection(lc)\n",
        " \n",
        "        sp.autoscale()\n",
        "        sp.margins(0.1)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emad3wwXzHdL"
      },
      "source": [
        "np_reader = np.load('/content/drive/MyDrive/Digai/d_alpha.npz', allow_pickle=True)\n",
        "d_alpha1, d_alpha2 = np_reader[\"arr_0\"], np_reader[\"arr_1\"]\n",
        "d_alpha = np.append(d_alpha1, d_alpha2).reshape((124800, 2))\n",
        "del d_alpha1, d_alpha2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LofJoxRuDR0n"
      },
      "source": [
        "nn = CreateNeuralNetwork(shape=(784, 256, 112, 26),\n",
        "                         initializer=Initializer.xavier(he=2),\n",
        "                         activation=ActivationFunction.relu(),\n",
        "                         output_activation=ActivationFunction.softmax())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7S83P_mM2Rb"
      },
      "source": [
        "nn = CreateNeuralNetwork(shape=(784, 256, 112, 26),\n",
        "                         initializer=Initializer.xavier(he=2),\n",
        "                         activation=ActivationFunction.relu(),\n",
        "                         output_activation=ActivationFunction.softmax())\n",
        " \n",
        "nn.train(training_set=d_alpha,\n",
        "         epochs=4,\n",
        "         batch_size=-1,\n",
        "         loss_function=LossFunction.mean_square(),\n",
        "         optimizer=Optimizer.decay(nn, 0.005),\n",
        "         vectorize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "F9Q-L7sBsBmR"
      },
      "source": [
        "nn.train(training_set=None,\n",
        "         epochs=1,\n",
        "         batch_size=None,\n",
        "         loss_function=None,\n",
        "         optimizer=Optimizer.learning_rate(nn, 0.001),\n",
        "         vectorize=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtkH4YlE3bIO"
      },
      "source": [
        "np.savez_compressed('alpha_nn_4.npz', nn.weights, nn.biases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS0pQkptDqHv"
      },
      "source": [
        "PlotGraph.plot_cost_graph(nn)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}